{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UoIlHg0c_Wza",
        "Hxdvlz6x_ZyZ",
        "aSeWf1n7_faS",
        "zNq6aTtqAjk6",
        "qswbIJq-B4y6",
        "cKkbKxI1EnUm",
        "lTmxCDrHF_wL",
        "uM9Xyfn7YOwu",
        "elYSiCc9cvXg",
        "wnTP6tqf1Oe0",
        "OhTEyiIxGlp9",
        "XWfoXcZlKppG",
        "78zTXhYt8A53",
        "CbAj0uxMJVDM",
        "VF2q0Tv2u7EI",
        "QMTEWC4_vE2-"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille-s/Sentence-similarity/blob/main/Lab_Week10_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EV EVALUATION\n",
        "\n"
      ],
      "metadata": {
        "id": "YbyXjAD1SlnC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EV- 0.0 Test sentence similarity models (optional)\n",
        "Just to play with a couple of models and a few sentences."
      ],
      "metadata": {
        "id": "sGGd1Fr3aOJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Transformers\n",
        "from IPython.display import clear_output\n",
        "! pip install datasets\\<4.0.0\n",
        "! pip install -U sentence-transformers\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "iq3LTFI1Y9L4",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "model = SentenceTransformer('nli-distilroberta-base-v2')\n",
        "\n",
        "clear_output()\n",
        "\n",
        "# Two lists of sentences\n",
        "sentences1 = ['The cat sits outside',\n",
        "             'A man is playing guitar',\n",
        "             'The new movie is awesome',\n",
        "              # Triples\n",
        "              # 'Aarhus_Airport | cityServed | Aarhus,_Denmark',\n",
        "              # 'Aarhus_Airport | location | Tirstrup',\n",
        "              # \"Textified\" triples\n",
        "              # 'Aarhus Airport city served Aarhus, Denmark',\n",
        "              # 'Aarhus Airport location Tirstrup'\n",
        "              ]\n",
        "\n",
        "sentences2 = ['The dog plays in the garden',\n",
        "              'A woman watches TV',\n",
        "              'The new movie is so great',\n",
        "              'Aarhus Airport serves the city of Aarhus, Denmark',\n",
        "              'Aarhus Airport is in Tirstrup',\n",
        "              'Aarhus Airport serves the city of Aarhus, Denmark',\n",
        "              'Aarhus Airport is in Tirstrup'\n",
        "              ]\n",
        "\n",
        "#Compute embedding for both lists\n",
        "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
        "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
        "\n",
        "#Compute cosine-similarities\n",
        "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
        "\n",
        "#Output the pairs with their score\n",
        "for i in range(len(sentences1)):\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
      ],
      "metadata": {
        "id": "hK9V6jXh1k8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**EV-1 Setup**"
      ],
      "metadata": {
        "id": "ZuOt4WNmKUTs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EV-1.1 Mount drive (optional)"
      ],
      "metadata": {
        "id": "UoIlHg0c_Wza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "t_zYDsQRKY0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EV-1.2 Download files"
      ],
      "metadata": {
        "id": "Hxdvlz6x_ZyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install --upgrade --no-cache-dir gdown\n",
        "from IPython.display import clear_output\n",
        "! gdown 1M2sxOCSUQJiLt6yC40fGI0QWSeq7jVod\n",
        "! unzip '/content/Lab_Week10.zip'\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "iGUUj86O49Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EV-1.3 Install transformers"
      ],
      "metadata": {
        "id": "aSeWf1n7_faS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install -U sentence-transformers\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "teJtXq-q90bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**EV-2 Load files and model**  (INPUT NEEDED: path to fine-tuned model)"
      ],
      "metadata": {
        "id": "8AAXgwfD-Zu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Path of fine-tuned model to evaluate\n",
        "fineTunedTransformer_path = ''#@param string"
      ],
      "metadata": {
        "cellView": "form",
        "id": "E7Cow4GkD_1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####EV-2.1 Load models, set paths"
      ],
      "metadata": {
        "id": "zNq6aTtqAjk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "files_path = '/content/Lab_Week10/Eval/'\n",
        "eval_scores_candidates = '/content/Lab_Week10/Eval/results_candSent/'\n",
        "\n",
        "# fineTunedTransformer_path = '/content/drive/MyDrive/Colab-dump/Lab_Week10/MyModel-nli-distilroberta-base-v2-2023-03-22_12-17-11'\n",
        "# fineTunedTransformer_path = '/content/drive/MyDrive/Colab-dump/MyModel-nli-distilroberta-base-v2-2023-03-16_12-47-24_W2020_noTypes-noTags_TrueCasedText'\n",
        "fineTuned_model = SentenceTransformer(fineTunedTransformer_path)\n",
        "offTheShelf_model = SentenceTransformer('nli-distilroberta-base-v2')\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "f_Gck0EkAPt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####EV-2.2 Load files"
      ],
      "metadata": {
        "id": "qswbIJq-B4y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "with open(files_path + 'candidateSentences_dev_1.txt', 'rb') as f:\n",
        "  allSentences = list(pickle.load(f))\n",
        "\n",
        "with open(files_path + 'targetSentences_dev_1.txt', 'rb') as f:\n",
        "  referenceSentences = list(pickle.load(f))\n",
        "\n",
        "with open(files_path + 'triples_dev_1_textified.txt', 'rb') as f:\n",
        "  textifiedTriples = pickle.load(f)"
      ],
      "metadata": {
        "id": "ln8qYf0lDE1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print('Expected: 1834, 401, 401')\n",
        "print('Expected: 961, 401, 401')\n",
        "print(len(allSentences), len(textifiedTriples), len(referenceSentences))\n",
        "print(referenceSentences[1], textifiedTriples[1])"
      ],
      "metadata": {
        "id": "DSplnV5q-9rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **EV-3 Scoring of candidate sentences with Fine-tuned model (use GPU)**\n",
        "Run for fine-tuned model only."
      ],
      "metadata": {
        "id": "muuIPRoXTS61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####EV-3.1 Scoring Functions"
      ],
      "metadata": {
        "id": "cKkbKxI1EnUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate(textified_triple, sentences_list, model):\n",
        "    to_embed = [[textified_triple], sentences_list]\n",
        "    to_embed = [element for sublist in to_embed for element in sublist]\n",
        "\n",
        "    embeddings = model.encode(to_embed, convert_to_tensor=True)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i in range(1, len(embeddings)):\n",
        "        similarity = float(util.pytorch_cos_sim(embeddings[0], embeddings[i])[0][0])\n",
        "        results.append([sentences_list[i-1], similarity])\n",
        "\n",
        "    results.sort(key=lambda result: result[1], reverse=True)\n",
        "    return results\n",
        "\n",
        "def evaluate_allTriples(textifiedTriples, sentences_list, savePath, model, ft=False):\n",
        "    for i, textified_triple in enumerate(tqdm(textifiedTriples)):\n",
        "        results = evaluate(textified_triple, sentences_list, model)\n",
        "        if ft == False:\n",
        "            with open(savePath + 'triple'+str(i)+'_results2.txt', 'wb') as fh:\n",
        "                pickle.dump(results, fh)\n",
        "        else:\n",
        "            with open(savePath + 'triple'+str(i)+'_results1.txt', 'wb') as fh:\n",
        "                pickle.dump(results, fh)\n",
        "\n",
        "def evaluate_allTriples2(textifiedTriples, referenceSentences, savePath, model, ft=False):\n",
        "    for i, textified_triple in enumerate(tqdm(textifiedTriples)):\n",
        "        results = evaluate(textified_triple, list(referenceSentences[i]), model)\n",
        "        if ft == False:\n",
        "            with open(savePath + 'triple'+str(i)+'_results2.txt', 'wb') as fh:\n",
        "                pickle.dump(results, fh)\n",
        "        else:\n",
        "            with open(savePath + 'triple'+str(i)+'_results1.txt', 'wb') as fh:\n",
        "                pickle.dump(results, fh)"
      ],
      "metadata": {
        "id": "hWUMoLKYEbmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####EV-3.2 Run scoring (takes about 5 minutes)"
      ],
      "metadata": {
        "id": "lTmxCDrHF_wL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The results for the off-the-shelf module are already provided, no need to run this cell\n",
        "# evaluate_allTriples(textifiedTriples, allSentences, eval_scores_candidates, offTheShelf_model)"
      ],
      "metadata": {
        "id": "sIW3573-GBwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_allTriples(textifiedTriples, allSentences, eval_scores_candidates, fineTuned_model, ft=True)"
      ],
      "metadata": {
        "id": "eYsPH7fzCdmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EV-3.3 Print raw results"
      ],
      "metadata": {
        "id": "uM9Xyfn7YOwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_result(textified_triple, results, n=None):\n",
        "    print('---', textified_triple, '---')\n",
        "    if not n or n > len(results):\n",
        "        n = len(results)\n",
        "    for i in range(n):\n",
        "   # for i, result in enumerate(results):\n",
        "        print(str(i+1)+'.', 'Similarity: {:.3f}'.format(results[i][1]), '->', results[i][0])\n",
        "\n",
        "def printResults(files_path, model, n):\n",
        "\n",
        "    results_path = eval_scores_candidates\n",
        "    with open (files_path + 'triples_dev_1_textified.txt', 'rb') as f:\n",
        "        textifiedTriples = pickle.load(f)\n",
        "\n",
        "    for i in range(n):\n",
        "\n",
        "        textified_triple = textifiedTriples[i]\n",
        "\n",
        "        if model == 'offTheShelf':\n",
        "            result2_name = 'triple'+str(i)+'_results2.txt'\n",
        "            pickle_off = open(results_path + result2_name, 'rb')\n",
        "            result2 = pickle.load(pickle_off)\n",
        "            print('OffTheShelf Model')\n",
        "            print_result(textified_triple, result2, n=10)\n",
        "            print()\n",
        "            print()\n",
        "\n",
        "        elif model =='fineTuned':\n",
        "            result1_name = 'triple'+str(i)+'_results1.txt'\n",
        "            pickle_off = open(results_path + result1_name, 'rb')\n",
        "            result1 = pickle.load(pickle_off)\n",
        "            print('FineTuned Model')\n",
        "            print_result(textified_triple, result1, n=10)\n",
        "            print()\n",
        "            print()\n",
        "\n",
        "        elif model =='both':\n",
        "            result1_name = 'triple'+str(i)+'_results1.txt'\n",
        "            result2_name = 'triple'+str(i)+'_results2.txt'\n",
        "            pickle_off = open(results_path + result1_name, 'rb')\n",
        "            result1 = pickle.load(pickle_off)\n",
        "            pickle_off = open(results_path + result2_name, 'rb')\n",
        "            result2 = pickle.load(pickle_off)\n",
        "            print('FineTuned Model')\n",
        "            print_result(textified_triple, result1, n=10)\n",
        "            print()\n",
        "            print('OffTheShelf Model')\n",
        "            print_result(textified_triple, result2, n=10)\n",
        "            print()\n",
        "            print()"
      ],
      "metadata": {
        "id": "6VJhGs6aYRpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Second parameter can be 'offTheShelf', 'fineTuned' or 'both'\n",
        "model = 'fineTuned'#@param ['offTheShelf', 'fineTuned', 'both']\n",
        "# Third parameter is the number of outputs to print (up to 401)\n",
        "printResults(files_path, 'offTheShelf', 2)"
      ],
      "metadata": {
        "id": "hGyjMnqNYYH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **EV-4 Evaluation results aggregation** (INPUT NEEDED: model to evaluate)\n",
        "Takes the files produced in the scoring phase and extracts global results of the model."
      ],
      "metadata": {
        "id": "Ec3sGTudTkzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EV-4.1 Functions and pre-processing"
      ],
      "metadata": {
        "id": "elYSiCc9cvXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import statistics\n",
        "\n",
        "# To define thresholds to be tested\n",
        "thresholds = []\n",
        "threshold = 0\n",
        "for i in range(99):\n",
        "    threshold += 0.01\n",
        "    thresholds.append(round(threshold, 2))\n",
        "\n",
        "\n",
        "def classify_results(results, threshold, correctSentences, chooseTest):\n",
        "    truePositives = 0\n",
        "    trueNegatives = 0\n",
        "    falsePositives = 0\n",
        "    falseNegatives = 0\n",
        "    for result in results:\n",
        "        if chooseTest == 'originalTriples':\n",
        "            if result[1] >= threshold:\n",
        "                if result[0] in correctSentences:\n",
        "                    truePositives += 1\n",
        "                else:\n",
        "                    falsePositives += 1\n",
        "            else:\n",
        "                if result[0] in correctSentences:\n",
        "                    falseNegatives += 1\n",
        "                else:\n",
        "                    trueNegatives += 1\n",
        "        else:\n",
        "            if result[1] >= threshold:\n",
        "                falsePositives += 1\n",
        "            else:\n",
        "                trueNegatives += 1\n",
        "\n",
        "    classifiedResults = [truePositives, falsePositives, trueNegatives, falseNegatives]\n",
        "\n",
        "    return classifiedResults\n",
        "\n",
        "def top_isCorrect(results, correctSentences):\n",
        "    top_isCorrect = True\n",
        "    top = [result[0] for result in results[:len(correctSentences)]]\n",
        "    for correctSentence in correctSentences:\n",
        "        if correctSentence not in top:\n",
        "            top_isCorrect = False\n",
        "            break\n",
        "    return top_isCorrect\n",
        "\n",
        "def get_correctSentencesMean(results, correctSentences, topIsCorrect):\n",
        "    if topIsCorrect:\n",
        "        correctSentencesMean = statistics.mean([result[1] for result in results[:len(correctSentences)]])\n",
        "    else:\n",
        "        correctSentencesScores = []\n",
        "        for correctSentence in correctSentences:\n",
        "            for result in results:\n",
        "                if result[0] == correctSentence:\n",
        "                    correctSentencesScores.append(result[1])\n",
        "        correctSentencesMean = statistics.mean(correctSentencesScores)\n",
        "\n",
        "    return correctSentencesMean\n",
        "\n",
        "def get_classificationMetrics(classifiedResults_sum, threshold_index):\n",
        "    tp = classifiedResults_sum[threshold_index][0]\n",
        "    fp = classifiedResults_sum[threshold_index][1]\n",
        "    tn = classifiedResults_sum[threshold_index][2]\n",
        "    fn = classifiedResults_sum[threshold_index][3]\n",
        "\n",
        "    if tp == 0:\n",
        "        precision = 0\n",
        "        recall = 0\n",
        "        f1 = 0\n",
        "    else:\n",
        "        precision = tp / (fp + tp)\n",
        "        recall = tp / (fn + tp)\n",
        "        f1 = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    accuracy = (fp + tn) / (tp + fn + tn + fp)\n",
        "\n",
        "    metrics = [precision, recall, accuracy, f1]\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def get_allClassificationMetrics(thresholds, classifiedResults_sum):\n",
        "\n",
        "    allMetrics = []\n",
        "\n",
        "    for i, threshold in enumerate(thresholds):\n",
        "        metrics = get_classificationMetrics(classifiedResults_sum, i)\n",
        "        allMetrics.append(metrics)\n",
        "\n",
        "    return allMetrics\n",
        "\n",
        "def get_allProperties(triples):\n",
        "    all_properties = set()\n",
        "    for triple in triples:\n",
        "        all_properties.add(triple[1])\n",
        "    return all_properties"
      ],
      "metadata": {
        "id": "PS6PpsUjT41z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open (files_path + 'triples_dev_1_textified.txt', 'rb') as f:\n",
        "#     textifiedTriples = pickle.load(f)\n",
        "\n",
        "# notParenthesesTriples = []\n",
        "# for triple in textifiedTriples:\n",
        "#     if not '(' in triple:\n",
        "#         notParenthesesTriples.append(triple)\n",
        "# print('Expected: 350, 401')\n",
        "# print(len(notParenthesesTriples), len(textifiedTriples))\n",
        "\n",
        "# with open (files_path + 'targetSentences_dev_1.txt', 'rb') as f:\n",
        "#     sentences = pickle.load(f)\n",
        "# print(textifiedTriples[111], sentences[111])"
      ],
      "metadata": {
        "id": "YVsvEGTTba3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "originalTriples_path2 = ''\n",
        "\n",
        "def checkResults(thresholds, n, files_path, chooseTest, model, subObjSentences=False):\n",
        "\n",
        "    if chooseTest == 'originalTriples':\n",
        "        if subObjSentences:\n",
        "            results_path = originalTriples_path2\n",
        "        else:\n",
        "            results_path = eval_scores_candidates\n",
        "        with open (files_path + 'triples_dev_1_textified.txt', 'rb') as f:\n",
        "            textifiedTriples = pickle.load(f)\n",
        "        with open (files_path + 'targetSentences_dev_1.txt', 'rb') as f:\n",
        "            sentences = pickle.load(f)\n",
        "        '''\n",
        "        notParenthesesIndexes = []\n",
        "        for i, triple in enumerate(textifiedTriples):\n",
        "            if not '(' in triple:\n",
        "                notParenthesesIndexes.append(i)\n",
        "        '''\n",
        "    else:\n",
        "        if chooseTest == 'exchangedObjSubTriples':\n",
        "            results_path = exchangedObjSubTriplesResults_path\n",
        "            with open (files_path + 'textified_exchangedObjSubTriples.txt', 'rb') as f:\n",
        "                textifiedTriples = pickle.load(f)\n",
        "\n",
        "        elif chooseTest == 'randomPropertyTriples':\n",
        "            results_path = randomPropertyTriplesResults_path\n",
        "            with open (files_path + 'textified_randomPropertyTriples.txt', 'rb') as f:\n",
        "                textifiedTriples = pickle.load(f)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"chooseTest must be one of the following: 'originalTriples', 'exchangedObjSubTriples', 'randomPropertyTriples'\")\n",
        "\n",
        "\n",
        "    with open (files_path + 'triples_dev_1_split.txt', 'rb') as f:\n",
        "        triples = pickle.load(f)\n",
        "    properties_set = get_allProperties(triples)\n",
        "    triples_properties = [triple[1] for triple in triples]\n",
        "    properties_errors = {property: 0 for property in properties_set}\n",
        "\n",
        "    '''with open (files_path + 'triples_categories.txt', 'rb') as f:\n",
        "        triples_categories = pickle.load(f)'''\n",
        "\n",
        "    highestScores = []\n",
        "    # offTheShelf_highestScores = []\n",
        "\n",
        "    '''categories_set = set(triples_categories)\n",
        "    categories_errors = {category: 0 for category in categories_set}'''\n",
        "\n",
        "    classifiedResults_sum = [[0]*4 for i in range(len(thresholds))]   #[truePositives, trueNegatives, falsePositives, falseNegatives] for each threshold\n",
        "    incorrectTops_sum = 0\n",
        "    incorrectTop1_sum = 0\n",
        "    correctSentences_means = []\n",
        "    differences = []\n",
        "\n",
        "    fineTuned_errors = []\n",
        "\n",
        "    #for i in tqdm(notParenthesesIndexes):\n",
        "    for i, textifiedTriple in enumerate(tqdm(textifiedTriples)):\n",
        "    #for i in tqdm(range(textifiedTriples)):\n",
        "\n",
        "        #textifiedTriple = textifiedTriples[i]\n",
        "        correctSentences = sentences[i] if chooseTest == 'originalTriples' else None\n",
        "\n",
        "        results1 = ''\n",
        "        results2 = ''\n",
        "        if model == 'fineTuned':\n",
        "            results1_name = 'triple'+str(i)+'_results1.txt'\n",
        "            pickle_off = open(results_path + results1_name, 'rb')\n",
        "            results1 = pickle.load(pickle_off)\n",
        "            highestScores.append(results1[0][1])\n",
        "            for j, threshold in enumerate(thresholds):\n",
        "                fineTuned_classifiedResults = classify_results(results1, threshold, correctSentences, chooseTest)\n",
        "                classifiedResults_sum[j] = [i+j for i,j in zip(classifiedResults_sum[j], fineTuned_classifiedResults)]\n",
        "\n",
        "        elif model == 'offTheShelf':\n",
        "            results2_name = 'triple'+str(i)+'_results2.txt'\n",
        "            pickle_off = open(results_path + results2_name, 'rb')\n",
        "            results2 = pickle.load(pickle_off)\n",
        "            highestScores.append(results2[0][1])\n",
        "            for j, threshold in enumerate(thresholds):\n",
        "                offTheShelf_classifiedResults = classify_results(results2, threshold, correctSentences, chooseTest)\n",
        "                classifiedResults_sum[j] = [i+j for i,j in zip(classifiedResults_sum[j], offTheShelf_classifiedResults)]\n",
        "\n",
        "\n",
        "        metrics = get_allClassificationMetrics(thresholds, classifiedResults_sum)\n",
        "\n",
        "        model_f1List = [[threshold, metrics[i][3]] for i, threshold in enumerate(thresholds)]\n",
        "        model_f1Top = sorted(model_f1List, key=lambda x: x[1], reverse=True)\n",
        "        # offTheShelf_f1List = [[threshold, offTheShelf_metrics[i][3]] for i, threshold in enumerate(thresholds)]\n",
        "        # offTheShelf_f1Top = sorted(offTheShelf_f1List, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        if chooseTest == 'originalTriples':\n",
        "            topIsCorrect = ''\n",
        "            if model == 'fineTuned':\n",
        "                topIsCorrect = top_isCorrect(results1, correctSentences)\n",
        "                correctSentencesMean = get_correctSentencesMean(results1, correctSentences, topIsCorrect)\n",
        "                if results1[0][0] not in correctSentences:\n",
        "                   incorrectTop1_sum += 1\n",
        "            elif model == 'offTheShelf':\n",
        "                topIsCorrect = top_isCorrect(results2, correctSentences)\n",
        "                correctSentencesMean = get_correctSentencesMean(results2, correctSentences, topIsCorrect)\n",
        "                if results2[0][0] not in correctSentences:\n",
        "                    incorrectTop1_sum += 1\n",
        "\n",
        "            correctSentences_means.append(correctSentencesMean)\n",
        "\n",
        "            difference = ''\n",
        "\n",
        "            if topIsCorrect:\n",
        "                if model == 'fineTuned':\n",
        "                    if len(correctSentences) < len(results1):\n",
        "                        difference = correctSentencesMean - results1[len(correctSentences)][1]\n",
        "                    else:\n",
        "                        difference = 0.212 ##\n",
        "                elif model == 'offTheShelf':\n",
        "                    if len(correctSentences) < len(results2):\n",
        "                        difference = correctSentencesMean - results2[len(correctSentences)][1]\n",
        "                    else:\n",
        "                        difference = 0.110\n",
        "            else:\n",
        "                incorrectTops_sum += 1\n",
        "                difference = None\n",
        "                if model == 'fineTuned':\n",
        "                   fineTuned_errors.append([textifiedTriple, correctSentences, results1[:5]])\n",
        "                '''categories_errors[triples_categories[i]] += 1'''\n",
        "                properties_errors[triples_properties[i]] += 1\n",
        "\n",
        "            if model == 'fineTuned':\n",
        "                correctSentences_means.append(get_correctSentencesMean(results1, correctSentences, topIsCorrect))\n",
        "            elif model == 'offTheShelf':\n",
        "                correctSentences_means.append(get_correctSentencesMean(results2, correctSentences, topIsCorrect))\n",
        "\n",
        "            differences.append(difference)\n",
        "\n",
        "    highestScores_mean = statistics.mean(highestScores)\n",
        "\n",
        "    if chooseTest == 'originalTriples':\n",
        "        '''category_errors = [[category, categories_errors[category]] for category in categories_errors]'''\n",
        "        property_errors = [[property, properties_errors[property]] for property in properties_errors]\n",
        "\n",
        "        incorrectTops_percentage = incorrectTops_sum / n * 100\n",
        "\n",
        "        incorrectTop1_percentage = incorrectTop1_sum / n * 100\n",
        "\n",
        "        correctSentencesMeans_mean = statistics.mean(correctSentences_means)\n",
        "\n",
        "        differences_mean = statistics.mean([difference for difference in filter(None, differences)])\n",
        "\n",
        "        results = [metrics,\n",
        "                model_f1Top,\n",
        "                classifiedResults_sum,\n",
        "                incorrectTops_percentage,\n",
        "                incorrectTop1_percentage,\n",
        "                correctSentencesMeans_mean,\n",
        "                differences_mean,\n",
        "                highestScores_mean,\n",
        "                fineTuned_errors, thresholds, property_errors]\n",
        "    else:\n",
        "        results = [metrics,\n",
        "                model_f1Top,\n",
        "                classifiedResults_sum,\n",
        "                highestScores_mean,\n",
        "                thresholds]\n",
        "\n",
        "    return results\n",
        "\n",
        "def print_infoResults(results, chooseTest, model):\n",
        "    if chooseTest == 'originalTriples':\n",
        "        [metrics,\n",
        "        model_f1Top,\n",
        "        classifiedResults_sum,\n",
        "        incorrectTops_percentage,\n",
        "        incorrectTop1_percentage,\n",
        "        correctSentencesMeans_mean,\n",
        "        differences_mean,\n",
        "        highestScores_mean,\n",
        "        fineTuned_errors, thresholds, property_errors] = results\n",
        "    else:\n",
        "         [metrics,\n",
        "          model_f1Top,\n",
        "          classifiedResults_sum,\n",
        "          highestScores_mean,\n",
        "          thresholds] = results\n",
        "\n",
        "    print('CLASSIFICATION METRICS')\n",
        "    for i, threshold in enumerate(thresholds):\n",
        "        print(threshold)\n",
        "        print('\\tEvaluated model:  ')\n",
        "        print('\\t    [True Positives, False Positives]: [{:.0f}, {:.0f}]'.format(classifiedResults_sum[i][0], classifiedResults_sum[i][1]))\n",
        "        print('\\t    [False Negatives, True Negatives]: [{:.0f}, {:.0f}]'.format(classifiedResults_sum[i][3], classifiedResults_sum[i][2]))\n",
        "        print('\\t\\tPrecision: {:.3f}'.format(metrics[i][0]))\n",
        "        print('\\t\\tRecall: {:.3f}'.format(metrics[i][1]))\n",
        "        print('\\t\\taccuracy: {:.3f}'.format(metrics[i][2]))\n",
        "        print('\\t\\tf1-score: {:.3f}'.format(metrics[i][3]))\n",
        "        print()\n",
        "    print('TOP F1 THRESHOLDS')\n",
        "    print('\\tEvaluated model:')\n",
        "    print('\\t\\t', model_f1Top[:5])\n",
        "    print()\n",
        "    print('HIGHEST SCORES MEAN')\n",
        "    print('Evaluated model:  {:.5f}'.format(highestScores_mean))\n",
        "    print()\n",
        "\n",
        "    if chooseTest == 'originalTriples':\n",
        "        print('NOT CORRECT TOPS')\n",
        "        print('Evaluated model:  {:.2f}%'.format(incorrectTops_percentage))\n",
        "        print()\n",
        "        print('NOT CORRECT TOP1')\n",
        "        print('Evaluated model:  {:.2f}%'.format(incorrectTop1_percentage))\n",
        "        print()\n",
        "        print('CORRECT SENTENCES SCORE MEANS MEAN')\n",
        "        print('Evaluated model: ', '{:.3f}'.format(correctSentencesMeans_mean))\n",
        "        print()\n",
        "        print('DIFFERENCE BETWEEN TOP SCORE MEANS AND FIRST NON CORRECT MEAN')\n",
        "        print('Evaluated model: ', '{:.3f}'.format(differences_mean))\n",
        "        print()\n",
        "        if model == 'fineTuned':\n",
        "            print('FINE-TUNED MODEL ERRORS')\n",
        "            for i, [textified_triple, correct_sentences, results] in enumerate(fineTuned_errors):\n",
        "                print(i+1, '---', textified_triple) #, '---  (category: '+category+')')\n",
        "                print('\\tCorrect Sentences')\n",
        "                for j, sentence in enumerate(correct_sentences):\n",
        "                    print('\\t\\t', j+1, sentence)\n",
        "                print('\\tTop sentences')\n",
        "                for j, result in enumerate(results):\n",
        "                    print('\\t\\t', j+1, result[0], '=>', result[1])\n",
        "                print()\n",
        "            print()\n",
        "        '''print('CATEGORY ERRORS')\n",
        "        for [category, sum] in category_errors:\n",
        "            print('\\t', category+':', sum)\n",
        "        print()'''\n",
        "        # print('PROPERTY ERRORS')\n",
        "        # for [property, sum] in property_errors:\n",
        "        #     print('\\t', property+':', sum)\n",
        "        # print()"
      ],
      "metadata": {
        "id": "5D62dsotceue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####EV-4.2 Run evaluation (pick model first)"
      ],
      "metadata": {
        "id": "WRmT1c62ig0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Pick one model type to evaluate\n",
        "model = 'offTheShelf'#@param ['offTheShelf', 'fineTuned']\n",
        "#'originalTriples', 'exchangedObjSubTriples', 'randomPropertyTriples'\n",
        "results = checkResults(thresholds, 401, files_path, 'originalTriples', model)\n",
        "print_infoResults(results, 'originalTriples', model)"
      ],
      "metadata": {
        "id": "hpHFpQ4acqz-",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FT FINE TUNING"
      ],
      "metadata": {
        "id": "9QCwCiBMrQxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "F7bgHF9gWuCg",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FT-1 Dataset**"
      ],
      "metadata": {
        "id": "Y1sMs-r5I2HT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FT-1.1 Load WebNLG v3.0"
      ],
      "metadata": {
        "id": "wnTP6tqf1Oe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For clearing outputs of installs\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# datasets is for loading datasets from HuggingFace\n",
        "! pip install datasets\\<4.0.0\n",
        "from datasets import load_dataset\n",
        "\n",
        "clear_output()\n",
        "\n",
        "# The following is deprecated and only works with pre-v4.0 of datasets.\n",
        "webnlg = load_dataset('web_nlg', 'release_v3.0_en', trust_remote_code=True)\n",
        "# New version of data but neeeds to be converted to previous format\n",
        "# webnlg = load_dataset(\"Aunderline/webnlg\", revision=\"refs/convert/parquet\")"
      ],
      "metadata": {
        "id": "xc0Zsb89VMq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load triples of size 1 from the WebNLG training data"
      ],
      "metadata": {
        "id": "3WzSE4YLYwB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_webnlg(webnlg):\n",
        "    dataset = []\n",
        "    for sample in webnlg['train']:\n",
        "        if sample['size'] == 1:\n",
        "            dataset.append([sample['modified_triple_sets']['mtriple_set'][0][0], sample['lex']['text']])\n",
        "    return dataset\n",
        "\n",
        "dataset = load_webnlg(webnlg)"
      ],
      "metadata": {
        "id": "5i4e5Gcf1VNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge repeated triples"
      ],
      "metadata": {
        "id": "J9fDcpJdYuSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_repeated_triples(dataset):\n",
        "\n",
        "    triples = [] #create a list of triples to then find the repeated ones\n",
        "    for sample in dataset:\n",
        "        triples.append(sample[0])\n",
        "\n",
        "    duplicates = [triple for triple in triples if triples.count(triple) > 1] #duplicated triples list\n",
        "\n",
        "    to_delete = []\n",
        "    for i, sample in enumerate(dataset): #Append sentences from repeated triples in the first occurrence triple list\n",
        "        triple = sample[0]\n",
        "        if triple in duplicates:\n",
        "            first_occurrence = triples.index(triple)\n",
        "            if i != first_occurrence:\n",
        "                for sentence in sample[1]:\n",
        "                    dataset[first_occurrence][1].append(sentence)\n",
        "                to_delete.append(i) #Save indexes of triples to delete\n",
        "\n",
        "    for i, index in enumerate(to_delete): #Delete repeated triples\n",
        "        del dataset[index - i]\n",
        "\n",
        "    for sample in dataset: #Remove repeated sentences after merging\n",
        "        sentence_set = set(sample[1])\n",
        "        # Sort so that sentences are always in the same order (important for getting the same sampling given a random seed later on)\n",
        "        sample[1] = sorted(list(sentence_set))\n",
        "\n",
        "    return dataset\n",
        "\n",
        "dataset = merge_repeated_triples(dataset)"
      ],
      "metadata": {
        "id": "IJYgIflGxx0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[0])\n",
        "print(len(dataset))"
      ],
      "metadata": {
        "id": "zPVK9MtlSghO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text triples: Each triple is a string in this form: \"Subject | Property | Object\""
      ],
      "metadata": {
        "id": "2sMPKHS1e1B1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_triples(dataset):\n",
        "    text_triples = []\n",
        "    for sample in dataset:\n",
        "        text_triples.append(sample[0])\n",
        "    return text_triples\n",
        "\n",
        "text_triples = get_text_triples(dataset)"
      ],
      "metadata": {
        "id": "_iAJ2l9H4h04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Num text triples:', len(text_triples), '(Expected: 3107)')\n",
        "print()\n",
        "print('text_triples[0]:')\n",
        "text_triples[0]"
      ],
      "metadata": {
        "id": "NJm5r_3pfd6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentences list: a list with all sentences corresponding to 1-triple inputs"
      ],
      "metadata": {
        "id": "zlwSl3QFGfdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentences(dataset):\n",
        "    sentences = []\n",
        "    for sample in dataset:\n",
        "        for sentence in sample[1]:\n",
        "            sentences.append(sentence)\n",
        "    return sentences\n",
        "\n",
        "sentences = get_sentences(dataset)"
      ],
      "metadata": {
        "id": "wzL4H8m2EOkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Num sentences:', len(sentences), '(Expected: 7630)')\n",
        "print()\n",
        "print('sentences[0]:')\n",
        "sentences[0]"
      ],
      "metadata": {
        "id": "21yAs7rVfgDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Triples list: triples are stored as a list of 3 elements to access Subj, Property and Obj separately"
      ],
      "metadata": {
        "id": "Q464VuorGixO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_triples(text_triples):\n",
        "    triples = []\n",
        "    for text_triple in text_triples:\n",
        "        firstBarIndex = text_triple.find('|')-1\n",
        "        secondBarIndex = text_triple.rfind('|')+2\n",
        "\n",
        "        sub = text_triple[:firstBarIndex]\n",
        "        prop = text_triple[firstBarIndex + 3 : secondBarIndex-3]\n",
        "        obj = text_triple[secondBarIndex:]\n",
        "\n",
        "        triple = [sub, prop, obj]\n",
        "        triples.append(triple)\n",
        "    return triples\n",
        "\n",
        "triples = get_triples(text_triples)"
      ],
      "metadata": {
        "id": "Jh46C-C99Z2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Num triples:', len(triples), '(Expected: 3107)')\n",
        "print()\n",
        "print('triples[0]:')\n",
        "triples[0]"
      ],
      "metadata": {
        "id": "iZN9yAL9fkOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save data\n",
        "# import pickle\n",
        "\n",
        "# fineTuningDataset_path = '/content/' #Write path to save the dataset\n",
        "# with open(fineTuningDataset_path + 'triples_split.txt', 'wb') as fh:\n",
        "#    pickle.dump(triples, fh)\n",
        "\n",
        "# with open(fineTuningDataset_path + 'triples_raw.txt', 'wb') as fh:\n",
        "#    pickle.dump(text_triples, fh)"
      ],
      "metadata": {
        "id": "DNEsR1EPP7_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FT-1.2 Extract info needed for Finetuning dataset: CELLS TO EDIT!"
      ],
      "metadata": {
        "id": "OhTEyiIxGlp9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct intermediate dataset: eventually, we want a dataset of sentence-like pairs with approximate similarity score between them."
      ],
      "metadata": {
        "id": "FuzgWOj45q7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL TO EDIT!\n",
        "# build a list of 2 dictionaries to store sentences with 2 different levels of similarity with the input triple\n",
        "# if you want to create more splits, add dict() to intermediate_dataset_temp, and adapt code in the cell to edit below.\n",
        "\n",
        "############################################################# EDIT BELOW #############################################################\n",
        "intermediate_dataset_temp = [dict(), dict()]\n",
        "############################################################# EDIT ABOVE #############################################################\n",
        "for category in intermediate_dataset_temp:\n",
        "    for text_triple in text_triples:\n",
        "        category[text_triple] = set()"
      ],
      "metadata": {
        "id": "LTM7UHzpwS_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(intermediate_dataset_temp[0]))\n",
        "print(intermediate_dataset_temp[0]['Aarhus_Airport | cityServed | \"Aarhus, Denmark\"'])"
      ],
      "metadata": {
        "id": "hY-wnl82USA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL TO EDIT!\n",
        "# For each triple, compare it to all other triples and get a set with the intersection of the two triples.\n",
        "for i, triple1 in enumerate(triples):\n",
        "    triple_1 = set(triple1)\n",
        "    for j, triple2 in enumerate(triples):\n",
        "        triple_2 = set(triple2)\n",
        "        final = triple_1 & triple_2\n",
        "\n",
        "        # Group together the sentences that verbalise triples that have all or no elements (Subj, Prop, Obj) in common with triple1\n",
        "        # Keep the subsets ordered so we can use the order later on to assign scores\n",
        "        # E.g. the data with NO overlap should be in the first position of the intermediate_dataset_temp list, and the data with full overlap should be in the last position.\n",
        "        ############################################################# EDIT BELOW #############################################################\n",
        "        if len(final) == 0:\n",
        "            for sentence in dataset[j][1]:\n",
        "                intermediate_dataset_temp[0][text_triples[i]].add(sentence)\n",
        "        elif len(final) == 3:\n",
        "            for sentence in dataset[j][1]:\n",
        "                intermediate_dataset_temp[1][text_triples[i]].add(sentence)\n",
        "        ############################################################# EDIT ABOVE #############################################################\n",
        "    # print(final)\n",
        "\n",
        "intermediate_dataset = []\n",
        "\n",
        "def listify_dicoValues(old_main_dico, category):\n",
        "    \"\"\" Transforms a set into a list\"\"\"\n",
        "    new_embedded_dico = {}\n",
        "    # dico[category] is a dictionary too\n",
        "    for input_triple in old_main_dico[category]:\n",
        "        # convert set into sorted list\n",
        "        sorted_list_sentences = sorted(list(old_main_dico[category][input_triple]))\n",
        "        #create a new data point in the intermediate dataset with for each triple of a given category a list of sentences\n",
        "        new_embedded_dico[input_triple] = sorted_list_sentences\n",
        "    return(new_embedded_dico)\n",
        "\n",
        "count = 0\n",
        "while count < len(intermediate_dataset_temp):\n",
        "    intermediate_dataset.append(listify_dicoValues(intermediate_dataset_temp, count))\n",
        "    count += 1"
      ],
      "metadata": {
        "id": "uX9r5dmK5y4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(len(intermediate_dataset_temp[0]))\n",
        "# print(list(intermediate_dataset_temp[0]['Aarhus_Airport | cityServed | \"Aarhus, Denmark\"'])[:10])\n",
        "# print(list(intermediate_dataset_temp[1]['Aarhus_Airport | cityServed | \"Aarhus, Denmark\"'])[:10])\n",
        "print(len(intermediate_dataset[0]))\n",
        "print(list(intermediate_dataset[0]['Aarhus_Airport | cityServed | \"Aarhus, Denmark\"'])[:10])\n",
        "print(list(intermediate_dataset[1]['Aarhus_Airport | cityServed | \"Aarhus, Denmark\"'])[:10])"
      ],
      "metadata": {
        "id": "upYe1J4yyA5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save dataset"
      ],
      "metadata": {
        "id": "3zsjMmqa3Ezd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "fineTuningDataset_path = '/content/' #Write path to save the dataset\n",
        "with open(fineTuningDataset_path + 'intermediate_dataset.txt', 'wb') as fh:\n",
        "   pickle.dump(intermediate_dataset, fh)"
      ],
      "metadata": {
        "id": "IjUFTGhr3qt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FT-1.3 Create text/string versions of the triples to be compared to the sentences"
      ],
      "metadata": {
        "id": "XWfoXcZlKppG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(triples[0])\n",
        "\n",
        "def camelCaseClean(text):\n",
        "  words = [[text[0]]]\n",
        "  for c in text[1:]:\n",
        "    if words[-1][-1].islower() and c.isupper():\n",
        "      words.append(list(c.lower()))\n",
        "    else:\n",
        "      words[-1].append(c)\n",
        "  words = [''.join(word) for word in words]\n",
        "  cleaned = ' '.join(words)\n",
        "  return cleaned\n",
        "\n",
        "textified_triples_noTypes_noTags = []\n",
        "\n",
        "for triple in triples:\n",
        "  # remove underscores and quotes\n",
        "  subj = triple[0].replace('_',' ').replace('\"','')\n",
        "  obj = triple[2].replace('_',' ').replace('\"','')\n",
        "  # split property names\n",
        "  prop = camelCaseClean(triple[1])\n",
        "\n",
        "  text = subj+' '+prop+' '+obj+' .'\n",
        "  textified_triples_noTypes_noTags.append(text)\n",
        "\n",
        "# print(textified_triples_noTypes_noTags[0])\n",
        "# print(len(textified_triples_noTypes_noTags))"
      ],
      "metadata": {
        "id": "17k0SGp2aLY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "fineTuningDataset_path = '/content/' #Write path to save the dataset\n",
        "with open(fineTuningDataset_path + 'triples_train_1_textified.txt', 'wb') as fh:\n",
        "   pickle.dump(textified_triples_noTypes_noTags, fh)"
      ],
      "metadata": {
        "id": "WnwmuJHCbG5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Free up memory\n",
        "import gc\n",
        "del dataset\n",
        "del intermediate_dataset\n",
        "del intermediate_dataset_temp\n",
        "del sentences\n",
        "del text_triples\n",
        "del textified_triples_noTypes_noTags\n",
        "del triples\n",
        "del webnlg\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "7cn6R9y3WIu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FT-1.4 Format dataset for Sentence Transformers: CELL TO EDIT!"
      ],
      "metadata": {
        "id": "78zTXhYt8A53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the files produced in the previous step"
      ],
      "metadata": {
        "id": "vSUhDeJ52NWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "!pip install -U sentence-transformers\n",
        "\n",
        "clear_output()\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Load created dataset, with triples and scores\n",
        "fineTuningDataset_path = '/content/' #Write path to save the dataset\n",
        "pickle_off = open(fineTuningDataset_path + 'intermediate_dataset.txt', 'rb')\n",
        "interm_dataset = pickle.load(pickle_off)\n",
        "\n",
        "# Load textified triples\n",
        "pickle_off = open(fineTuningDataset_path + 'triples_train_1_textified.txt', 'rb')\n",
        "textified_triples = pickle.load(pickle_off)"
      ],
      "metadata": {
        "id": "L6d2vlGr4WHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print some examples\n",
        "sample_triples = ['Aarhus_Airport | cityServed | \"Aarhus, Denmark\"', 'Aarhus_Airport | cityServed | Aarhus', 'Christian_Burns | genre | House_music', 'Athens_International_Airport | runwayLength | 3800.0', 'New_York_City | country | United_States']\n",
        "\n",
        "for triple in sample_triples:\n",
        "  print(triple)\n",
        "  print(list(interm_dataset[1][triple])[:10])\n",
        "  print(list(interm_dataset[0][triple])[:10])\n",
        "  print()\n",
        "\n",
        "print(len(textified_triples))\n",
        "print(textified_triples[0])"
      ],
      "metadata": {
        "id": "JShphJNPkqE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final dataset is a list of objects with two attributes, \"texts\", which is a list of two sentences (texts=[sentence1, sentence2]), and \"label\", which is the similarity score between these two sentences (label=score)."
      ],
      "metadata": {
        "id": "_m6FrVlMHC6G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ec5yjbssPX3"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from sentence_transformers import InputExample\n",
        "\n",
        "triples = list(interm_dataset[0].keys())\n",
        "# triples[999], textified_triples[999]\n",
        "\n",
        "# num_negative_samples = 0\n",
        "# num_positive_samples = 0\n",
        "\n",
        "# Build a list that contains the number of samples for each category\n",
        "# E.g. in our example: [23252141, 7645] = 23252141 negative samples and 7645 positive samples\n",
        "num_samples = []\n",
        "\n",
        "ft_sentencePair_score_list = []\n",
        "#visualize_examples = []\n",
        "for i, category in enumerate(interm_dataset):\n",
        "    # We assume that the position of a data split correlates with its level of similarity between triples and text, so we get the score from that position\n",
        "    # score must be between 0 and 1, see https://www.sbert.net/examples/training/sts/README.html, which is why we normalise it below\n",
        "    score = float(i/(len(interm_dataset)-1))\n",
        "    print('Category', i, end=': ')\n",
        "    num_samples.append(0)\n",
        "    for j, triple in enumerate(tqdm(triples)):\n",
        "        for sentence in category[triple]:\n",
        "            sentence1 = textified_triples[j]\n",
        "            sentence2 = sentence\n",
        "            inp_example = InputExample(texts=[sentence1, sentence2], label=score)\n",
        "            #visualize_example = [[sentence1, sentence2], score]\n",
        "            ft_sentencePair_score_list.append(inp_example)\n",
        "            #visualize_examples.append(visualize_example)\n",
        "            num_samples[i] += 1\n",
        "            # if i == 0:\n",
        "            #     num_negative_samples += 1\n",
        "            # elif i == 1:\n",
        "            #     num_positive_samples += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLNou-orz9BR"
      },
      "outputs": [],
      "source": [
        "# print(len(ft_sentencePair_score_list) == num_negative_samples + num_positive_samples)\n",
        "print(len(ft_sentencePair_score_list) == sum(num_samples))\n",
        "print(ft_sentencePair_score_list[0])\n",
        "print(num_samples) # 2 splits (no overlap and full overlap): [23252141, 7645]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Balance the dataset so as to have as many examples of positive and negative data."
      ],
      "metadata": {
        "id": "ynoSnanfi4gg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iLe5wvs_dc7"
      },
      "outputs": [],
      "source": [
        "# CELL TO EDIT!\n",
        "import random\n",
        "\n",
        "# category0 = ft_sentencePair_score_list[:num_negative_samples] # [:23252141]\n",
        "# category1 = ft_sentencePair_score_list[num_negative_samples:]\n",
        "\n",
        "# Get the smallest data split\n",
        "largest_split = 0\n",
        "for num_sample in num_samples:\n",
        "    if num_sample > largest_split:\n",
        "        largest_split = num_sample\n",
        "smallest_split = largest_split\n",
        "for num_sample in num_samples:\n",
        "    if num_sample < smallest_split:\n",
        "       smallest_split = num_sample\n",
        "\n",
        "# Get the position of the last data point of each split\n",
        "ids_last_element_of_splits = [num_samples[0]]\n",
        "separator = 1\n",
        "while separator < len(num_samples):\n",
        "    # Sum the previous and the new separator\n",
        "    new_separator = ids_last_element_of_splits[-1]+num_samples[separator]\n",
        "    ids_last_element_of_splits.append(new_separator)\n",
        "    separator += 1\n",
        "\n",
        "############################################################# EDIT BELOW #############################################################\n",
        "category0 = ft_sentencePair_score_list[:ids_last_element_of_splits[0]] # [:23252141]\n",
        "category1 = ft_sentencePair_score_list[ids_last_element_of_splits[0]:ids_last_element_of_splits[1]]\n",
        "\n",
        "random.seed(42)\n",
        "shuffled_category0 = random.sample(category0, len(category0))\n",
        "shuffled_category1 = random.sample(category1, len(category1))\n",
        "\n",
        "category0 = shuffled_category0[:smallest_split]\n",
        "category1 = shuffled_category1[:smallest_split]\n",
        "\n",
        "ft_sentencePair_score_list_new = category0 + category1\n",
        "############################################################# EDIT ABOVE #############################################################\n",
        "\n",
        "# print(category0[0].texts)\n",
        "# print(ft_sentencePair_score_list[0].texts)\n",
        "# print(ft_sentencePair_score_list[0].label)\n",
        "# print(ft_sentencePair_score_list[1000].texts)\n",
        "# print(ft_sentencePair_score_list[1000].label)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(ft_sentencePair_score_list_new))\n",
        "print(ids_last_element_of_splits)\n",
        "# x = 0\n",
        "# while x < 100:\n",
        "#   print(str(ft_sentencePair_score_list_new[x].label)+': '+str(ft_sentencePair_score_list_new[x].texts))\n",
        "#   x += 1\n",
        "\n",
        "# print(largest_split)\n",
        "# print(smallest_split)\n",
        "# print(ids_last_element_of_splits)\n",
        "# print(num_samples[0])\n",
        "# print(num_samples[0]+num_samples[1])\n",
        "# print(num_samples[0]+num_samples[1]+num_samples[2])\n",
        "# print(num_samples[0]+num_samples[1]+num_samples[2]+num_samples[3])"
      ],
      "metadata": {
        "id": "KoV-GOr05KTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRtV6VneFUC_"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(fineTuningDataset_path + 'fine_tuning_dataset.txt', 'wb') as fh:\n",
        "   pickle.dump(ft_sentencePair_score_list_new, fh)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Free up memory and Load created dataset if you want to have a look at it"
      ],
      "metadata": {
        "id": "IEzVSLiZ099K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "fineTuningDataset_path = '/content/'\n",
        "pickle_off = open(fineTuningDataset_path + 'fine_tuning_dataset.txt', 'rb')\n",
        "new_dataset = pickle.load(pickle_off)"
      ],
      "metadata": {
        "id": "SyEhR-MXzztq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Should print the last datapoints of a split and the first of the next split (different label)\n",
        "# print(new_dataset[smallest_split-1].texts)\n",
        "# print(new_dataset[smallest_split-1].label)\n",
        "# print(new_dataset[smallest_split].texts)\n",
        "# print(new_dataset[smallest_split].label)\n",
        "\n",
        "x = 0\n",
        "while x < len(new_dataset):\n",
        "  datapoint = new_dataset[x]\n",
        "  print(datapoint.texts)\n",
        "  print(datapoint.label)\n",
        "  x += smallest_split\n",
        "print()"
      ],
      "metadata": {
        "id": "5l_5NOXW0A7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Free up memory (needed for the whole notebook to run)\n",
        "import gc\n",
        "del category0\n",
        "del category1\n",
        "del ft_sentencePair_score_list\n",
        "del ft_sentencePair_score_list_new\n",
        "del interm_dataset\n",
        "del new_dataset\n",
        "del pickle_off\n",
        "del shuffled_category0\n",
        "del textified_triples\n",
        "del triples\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "w-VVfSH5RHVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FT-2 Fine-tuning**"
      ],
      "metadata": {
        "id": "KdZ5qi0WI5hS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FT-2.1 Set parameters, load model (INPUT NEEDED: path to save fine-tuned model)"
      ],
      "metadata": {
        "id": "CbAj0uxMJVDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL TO EDIT!\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import math\n",
        "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, util, InputExample\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import os\n",
        "import gzip\n",
        "import csv\n",
        "\n",
        "#### Just some code to print debug information to stdout\n",
        "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
        "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
        "                    level=logging.INFO,\n",
        "                    handlers=[LoggingHandler()])\n",
        "#### /print debug information to stdout\n",
        "\n",
        "#Check if dataset exsist. If not, download and extract  it\n",
        "'''sts_dataset_path = 'datasets/stsbenchmark.tsv.gz'\n",
        "\n",
        "if not os.path.exists(sts_dataset_path):\n",
        "    util.http_get('https://sbert.net/datasets/stsbenchmark.tsv.gz', sts_dataset_path)\n",
        "'''\n",
        "\n",
        "############################################################# EDIT BELOW #############################################################\n",
        "# Read the dataset\n",
        "model_name = 'nli-distilroberta-base-v2'\n",
        "# Set random seed\n",
        "torch.manual_seed(42)\n",
        "# 128 uses much more memory but doesn't seem to be faster\n",
        "train_batch_size = 64\n",
        "num_epochs = 4\n",
        "model_save_path = '/content/drive/MyDrive/Colab-dump/Lab_Week10/MyModel-'+model_name+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") #finetuned model path and name\n",
        "############################################################# EDIT ABOVE #############################################################\n",
        "\n",
        "# Load a pre-trained sentence transformer model\n",
        "model = SentenceTransformer(model_name)"
      ],
      "metadata": {
        "id": "Jw0yIMMeI9TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FT-2.2 Load dataset and create splits"
      ],
      "metadata": {
        "id": "VF2q0Tv2u7EI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "fineTuningDataset_path = '/content/'\n",
        "pickle_off = open(fineTuningDataset_path + 'fine_tuning_dataset.txt', 'rb')\n",
        "input_examples = pickle.load(pickle_off)"
      ],
      "metadata": {
        "id": "MJv6bqHZJn9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL TO EDIT!\n",
        "import random\n",
        "\n",
        "############################################################# EDIT BELOW #############################################################\n",
        "random.seed(42)\n",
        "shuffled_input_examples = random.sample(input_examples, len(input_examples)) # shuffles the ordering of filenames (deterministic given the chosen seed)\n",
        "\n",
        "split_1 = int(0.70 * len(shuffled_input_examples))\n",
        "split_2 = int(0.85 * len(shuffled_input_examples))\n",
        "############################################################# EDIT ABOVE #############################################################\n",
        "\n",
        "train_samples = shuffled_input_examples[:split_1]\n",
        "dev_samples = shuffled_input_examples[split_1:split_2]\n",
        "test_samples = shuffled_input_examples[split_2:]"
      ],
      "metadata": {
        "id": "XziL6jBlJ9SK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_samples), len(dev_samples), len(test_samples)"
      ],
      "metadata": {
        "id": "B3ZmXqlVKAYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FT-2.3 Create model (use GPU)"
      ],
      "metadata": {
        "id": "QMTEWC4_vE2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n",
        "train_loss = losses.CosineSimilarityLoss(model=model)\n",
        "\n",
        "# Development set: Measure correlation between cosine score and gold labels\n",
        "logging.info(\"Read STSbenchmark dev dataset\")\n",
        "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, name='sts-dev')\n",
        "\n",
        "# Configure the training. We skip evaluation in this example\n",
        "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n",
        "logging.info(\"Warmup-steps: {}\".format(warmup_steps))"
      ],
      "metadata": {
        "id": "OwovriXfKCY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model (Use GPU)\n",
        "import wandb\n",
        "# To avoid error messages or API key requests\n",
        "wandb.init(mode=\"disabled\")\n",
        "# This one works too but will be deprecated soon\n",
        "#os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
        "          evaluator=evaluator,\n",
        "          epochs=num_epochs,\n",
        "          evaluation_steps=1000,\n",
        "          warmup_steps=warmup_steps,\n",
        "          output_path=model_save_path)"
      ],
      "metadata": {
        "id": "tk9qmyIIKEcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Free up memory\n",
        "import gc\n",
        "del input_examples\n",
        "del dev_samples\n",
        "del model\n",
        "del test_samples\n",
        "del train_samples\n",
        "del shuffled_input_examples\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "fcZCOBLiTs5A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}