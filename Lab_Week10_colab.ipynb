{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cKkbKxI1EnUm"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille-s/Sentence-similarity/blob/main/Lab_Week10_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EV EVALUATION\n",
        "\n"
      ],
      "metadata": {
        "id": "YbyXjAD1SlnC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EV- 0.0 Test sentence similarity models (optional)\n",
        "Just to play with a couple of models and a few sentences."
      ],
      "metadata": {
        "id": "sGGd1Fr3aOJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Transformers\n",
        "from IPython.display import clear_output\n",
        "! pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "iq3LTFI1Y9L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "model = SentenceTransformer('nli-distilroberta-base-v2')\n",
        "\n",
        "clear_output()\n",
        "\n",
        "# Two lists of sentences\n",
        "sentences1 = ['The cat sits outside',\n",
        "             'A man is playing guitar',\n",
        "             'The new movie is awesome',\n",
        "              # Triples\n",
        "              'Aarhus_Airport | cityServed | Aarhus,_Denmark',\n",
        "              'Aarhus_Airport | location | Tirstrup',\n",
        "              # \"Textified\" triples\n",
        "              'Aarhus Airport city served Aarhus, Denmark',\n",
        "              'Aarhus Airport location Tirstrup'\n",
        "              ]\n",
        "\n",
        "sentences2 = ['The dog plays in the garden',\n",
        "              'A woman watches TV',\n",
        "              'The new movie is so great',\n",
        "              'Aarhus Airport serves the city of Aarhus, Denmark',\n",
        "              'Aarhus Airport is in Tirstrup',\n",
        "              'Aarhus Airport serves the city of Aarhus, Denmark',\n",
        "              'Aarhus Airport is in Tirstrup'\n",
        "              ]\n",
        "\n",
        "#Compute embedding for both lists\n",
        "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
        "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
        "\n",
        "#Compute cosine-similarities\n",
        "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
        "\n",
        "#Output the pairs with their score\n",
        "for i in range(len(sentences1)):\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hK9V6jXh1k8k",
        "outputId": "8d795568-6191-468e-b3cd-e109afb2bc3c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cat sits outside \t\t The dog plays in the garden \t\t Score: 0.2289\n",
            "A man is playing guitar \t\t A woman watches TV \t\t Score: -0.0171\n",
            "The new movie is awesome \t\t The new movie is so great \t\t Score: 0.9504\n",
            "Aarhus_Airport | cityServed | Aarhus,_Denmark \t\t Aarhus Airport serves the city of Aarhus, Denmark \t\t Score: 0.8245\n",
            "Aarhus_Airport | location | Tirstrup \t\t Aarhus Airport is in Tirstrup \t\t Score: 0.7943\n",
            "Aarhus Airport city served Aarhus, Denmark \t\t Aarhus Airport serves the city of Aarhus, Denmark \t\t Score: 0.9802\n",
            "Aarhus Airport location Tirstrup \t\t Aarhus Airport is in Tirstrup \t\t Score: 0.9270\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**EV-1 Setup**"
      ],
      "metadata": {
        "id": "ZuOt4WNmKUTs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EV-1.1 Mount drive (optional)"
      ],
      "metadata": {
        "id": "UoIlHg0c_Wza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "t_zYDsQRKY0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EV-1.2 Download files"
      ],
      "metadata": {
        "id": "Hxdvlz6x_ZyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install --upgrade --no-cache-dir gdown\n",
        "from IPython.display import clear_output\n",
        "! gdown 1M2sxOCSUQJiLt6yC40fGI0QWSeq7jVod\n",
        "! unzip '/content/Lab_Week10.zip'\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "iGUUj86O49Ux"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EV-1.3 Install transformers"
      ],
      "metadata": {
        "id": "aSeWf1n7_faS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install -U sentence-transformers\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "teJtXq-q90bK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####EV-1.4 Load models, set paths (INPUT NEEDED: path to fine-tuned model)"
      ],
      "metadata": {
        "id": "zNq6aTtqAjk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "files_path = '/content/Lab_Week10/Eval/'\n",
        "eval_scores_candidates = '/content/Lab_Week10/Eval/results_candSent/'\n",
        "eval_scores_target = '/content/Lab_Week10/Eval/results_targetSent/'\n",
        "\n",
        "# fineTunedTransformer_path = '/content/drive/MyDrive/Colab-dump/Lab_Week10/MyModel-nli-distilroberta-base-v2-2023-03-22_12-17-11'\n",
        "# fineTuned_model = SentenceTransformer(fineTunedTransformer_path)\n",
        "offTheShelf_model = SentenceTransformer('nli-distilroberta-base-v2')\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "f_Gck0EkAPt7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####EV-1.5 Load files"
      ],
      "metadata": {
        "id": "qswbIJq-B4y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "with open(files_path + 'candidateSentences_dev_1-2.txt', 'rb') as f:\n",
        "  allSentences = list(pickle.load(f))\n",
        "\n",
        "with open(files_path + 'targetSentences_dev_1.txt', 'rb') as f:\n",
        "  referenceSentences = list(pickle.load(f))\n",
        "\n",
        "with open(files_path + 'triples_dev_1_textified.txt', 'rb') as f:\n",
        "  textifiedTriples = pickle.load(f)"
      ],
      "metadata": {
        "id": "ln8qYf0lDE1t"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Expected: 1834, 401, 401')\n",
        "print(len(allSentences), len(textifiedTriples), len(referenceSentences))\n",
        "print(referenceSentences[1], textifiedTriples[1])"
      ],
      "metadata": {
        "id": "DSplnV5q-9rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **EV-2 Scoring of candidate sentences (use GPU)**\n",
        "Run for fine-tuned model only. "
      ],
      "metadata": {
        "id": "muuIPRoXTS61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####EV-2.1 Scoring Functions"
      ],
      "metadata": {
        "id": "cKkbKxI1EnUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate(textified_triple, sentences_list, model):\n",
        "    to_embed = [[textified_triple], sentences_list]\n",
        "    to_embed = [element for sublist in to_embed for element in sublist]\n",
        "\n",
        "    embeddings = model.encode(to_embed, convert_to_tensor=True)\n",
        "    \n",
        "    results = []\n",
        "\n",
        "    for i in range(1, len(embeddings)):\n",
        "        similarity = float(util.pytorch_cos_sim(embeddings[0], embeddings[i])[0][0])\n",
        "        results.append([sentences_list[i-1], similarity])\n",
        "\n",
        "    results.sort(key=lambda result: result[1], reverse=True)\n",
        "    return results\n",
        "\n",
        "def evaluate_allTriples(textifiedTriples, sentences_list, savePath, model, ft=False):\n",
        "    for i, textified_triple in enumerate(tqdm(textifiedTriples)):\n",
        "        results = evaluate(textified_triple, sentences_list, model)\n",
        "        if ft == False:\n",
        "            with open(savePath + 'triple'+str(i)+'_results2.txt', 'wb') as fh:\n",
        "                pickle.dump(results, fh)\n",
        "        else:\n",
        "            with open(savePath + 'triple'+str(i)+'_results1.txt', 'wb') as fh:\n",
        "                pickle.dump(results, fh)\n",
        "\n",
        "def evaluate_allTriples2(textifiedTriples, referenceSentences, savePath, model, ft=False):\n",
        "    for i, textified_triple in enumerate(tqdm(textifiedTriples)):\n",
        "        results = evaluate(textified_triple, list(referenceSentences[i]), model)\n",
        "        if ft == False:\n",
        "            with open(savePath + 'triple'+str(i)+'_results2.txt', 'wb') as fh:\n",
        "                pickle.dump(results, fh)\n",
        "        else:\n",
        "            with open(savePath + 'triple'+str(i)+'_results1.txt', 'wb') as fh:\n",
        "                pickle.dump(results, fh)"
      ],
      "metadata": {
        "id": "hWUMoLKYEbmt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####EV-2.2 Run scoring"
      ],
      "metadata": {
        "id": "lTmxCDrHF_wL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The results for the off-the-shelf module are already provided, no need to run this cell\n",
        "# evaluate_allTriples(textifiedTriples, allSentences, eval_scores_candidates, offTheShelf_model)"
      ],
      "metadata": {
        "id": "sIW3573-GBwr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_allTriples(textifiedTriples, allSentences, eval_scores_candidates, fineTuned_model, ft=True)"
      ],
      "metadata": {
        "id": "eYsPH7fzCdmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EV-2.3 Print raw results"
      ],
      "metadata": {
        "id": "uM9Xyfn7YOwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_result(textified_triple, results, n=None):\n",
        "    print('---', textified_triple, '---')\n",
        "    if not n or n > len(results):\n",
        "        n = len(results)\n",
        "    for i in range(n):\n",
        "   # for i, result in enumerate(results):\n",
        "        print(str(i+1)+'.', 'Similarity: {:.3f}'.format(results[i][1]), '->', results[i][0])\n",
        "\n",
        "def printResults(files_path, model, n):\n",
        "\n",
        "    results_path = eval_scores_candidates\n",
        "    with open (files_path + 'triples_dev_1_textified.txt', 'rb') as f:\n",
        "        textifiedTriples = pickle.load(f)\n",
        "\n",
        "    for i in range(n):\n",
        "        \n",
        "        textified_triple = textifiedTriples[i]\n",
        "        \n",
        "        if model == 'offTheShelf':\n",
        "            result2_name = 'triple'+str(i)+'_results2.txt'\n",
        "            pickle_off = open(results_path + result2_name, 'rb')\n",
        "            result2 = pickle.load(pickle_off)\n",
        "            print('OffTheShelf Model')\n",
        "            print_result(textified_triple, result2, n=10)\n",
        "            print()\n",
        "            print()\n",
        "        \n",
        "        elif model =='fineTuned':\n",
        "            result1_name = 'triple'+str(i)+'_results1.txt'\n",
        "            pickle_off = open(results_path + result1_name, 'rb')\n",
        "            result1 = pickle.load(pickle_off)    \n",
        "            print('FineTuned Model')\n",
        "            print_result(textified_triple, result1, n=10)\n",
        "            print()\n",
        "            print()\n",
        "        \n",
        "        elif model =='both':\n",
        "            result1_name = 'triple'+str(i)+'_results1.txt'\n",
        "            result2_name = 'triple'+str(i)+'_results2.txt'\n",
        "            pickle_off = open(results_path + result1_name, 'rb')\n",
        "            result1 = pickle.load(pickle_off)    \n",
        "            pickle_off = open(results_path + result2_name, 'rb')\n",
        "            result2 = pickle.load(pickle_off)\n",
        "            print('FineTuned Model')\n",
        "            print_result(textified_triple, result1, n=10)\n",
        "            print()\n",
        "            print('OffTheShelf Model')\n",
        "            print_result(textified_triple, result2, n=10)\n",
        "            print()\n",
        "            print()"
      ],
      "metadata": {
        "id": "6VJhGs6aYRpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Second parameter can be 'offTheShelf', 'fineTuned' or 'both'\n",
        "# Third parameter is the number of outputs to print (up to 401)\n",
        "printResults(files_path, 'offTheShelf', 2)"
      ],
      "metadata": {
        "id": "hGyjMnqNYYH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **EV-3 Evaluation results aggregation**\n",
        "Takes the files produced in the scoring phase and extracts global results of the model."
      ],
      "metadata": {
        "id": "Ec3sGTudTkzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EV-3.1 Functions and pre-processing"
      ],
      "metadata": {
        "id": "elYSiCc9cvXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import statistics\n",
        "\n",
        "# To define thresholds to be tested\n",
        "thresholds = []\n",
        "threshold = 0\n",
        "for i in range(99):\n",
        "    threshold += 0.01\n",
        "    thresholds.append(round(threshold, 2))\n",
        "\n",
        "\n",
        "def classify_results(results, threshold, correctSentences, chooseTest):\n",
        "    truePositives = 0\n",
        "    trueNegatives = 0\n",
        "    falsePositives = 0\n",
        "    falseNegatives = 0\n",
        "    for result in results:\n",
        "        if chooseTest == 'originalTriples':\n",
        "            if result[1] >= threshold:\n",
        "                if result[0] in correctSentences:\n",
        "                    truePositives += 1\n",
        "                else:\n",
        "                    falsePositives += 1\n",
        "            else:\n",
        "                if result[0] in correctSentences:\n",
        "                    falseNegatives += 1\n",
        "                else:\n",
        "                    trueNegatives += 1\n",
        "        else:\n",
        "            if result[1] >= threshold:\n",
        "                falsePositives += 1\n",
        "            else:\n",
        "                trueNegatives += 1\n",
        "\n",
        "    classifiedResults = [truePositives, falsePositives, trueNegatives, falseNegatives]\n",
        "\n",
        "    return classifiedResults\n",
        "\n",
        "def top_isCorrect(results, correctSentences):\n",
        "    top_isCorrect = True\n",
        "    top = [result[0] for result in results[:len(correctSentences)]]\n",
        "    for correctSentence in correctSentences:\n",
        "        if correctSentence not in top:\n",
        "            top_isCorrect = False\n",
        "            break\n",
        "    return top_isCorrect\n",
        "\n",
        "def get_correctSentencesMean(results, correctSentences, topIsCorrect):\n",
        "    if topIsCorrect:\n",
        "        correctSentencesMean = statistics.mean([result[1] for result in results[:len(correctSentences)]])\n",
        "    else:\n",
        "        correctSentencesScores = []\n",
        "        for correctSentence in correctSentences:\n",
        "            for result in results:\n",
        "                if result[0] == correctSentence:\n",
        "                    correctSentencesScores.append(result[1])\n",
        "        correctSentencesMean = statistics.mean(correctSentencesScores)\n",
        "\n",
        "    return correctSentencesMean\n",
        "\n",
        "def get_classificationMetrics(classifiedResults_sum, threshold_index):\n",
        "    tp = classifiedResults_sum[threshold_index][0]\n",
        "    fp = classifiedResults_sum[threshold_index][1]\n",
        "    tn = classifiedResults_sum[threshold_index][2]\n",
        "    fn = classifiedResults_sum[threshold_index][3]\n",
        "\n",
        "    if tp == 0:\n",
        "        precision = 0\n",
        "        recall = 0\n",
        "        f1 = 0\n",
        "    else:\n",
        "        precision = tp / (fp + tp)\n",
        "        recall = tp / (fn + tp)\n",
        "        f1 = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    accuracy = (fp + tn) / (tp + fn + tn + fp)\n",
        "\n",
        "    metrics = [precision, recall, accuracy, f1]\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def get_allClassificationMetrics(thresholds, classifiedResults_sum):\n",
        "    \n",
        "    allMetrics = []\n",
        "\n",
        "    for i, threshold in enumerate(thresholds):\n",
        "        metrics = get_classificationMetrics(classifiedResults_sum, i)\n",
        "        allMetrics.append(metrics)\n",
        "\n",
        "    return allMetrics\n",
        "\n",
        "def get_allProperties(triples):\n",
        "    all_properties = set()\n",
        "    for triple in triples:\n",
        "        all_properties.add(triple[1])\n",
        "    return all_properties"
      ],
      "metadata": {
        "id": "PS6PpsUjT41z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open (files_path + 'triples_dev_1_textified.txt', 'rb') as f:\n",
        "#     textifiedTriples = pickle.load(f)\n",
        "\n",
        "# notParenthesesTriples = []\n",
        "# for triple in textifiedTriples:\n",
        "#     if not '(' in triple:\n",
        "#         notParenthesesTriples.append(triple)\n",
        "# print('Expected: 350, 401')\n",
        "# print(len(notParenthesesTriples), len(textifiedTriples))\n",
        "\n",
        "# with open (files_path + 'targetSentences_dev_1.txt', 'rb') as f:\n",
        "#     sentences = pickle.load(f)\n",
        "# print(textifiedTriples[111], sentences[111])"
      ],
      "metadata": {
        "id": "YVsvEGTTba3e"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "originalTriples_path2 = ''\n",
        "\n",
        "def checkResults(thresholds, n, files_path, chooseTest, model, subObjSentences=False):\n",
        "\n",
        "    if chooseTest == 'originalTriples':\n",
        "        if subObjSentences:\n",
        "            results_path = originalTriples_path2\n",
        "        else:\n",
        "            results_path = eval_scores_candidates\n",
        "        with open (files_path + 'triples_dev_1_textified.txt', 'rb') as f:\n",
        "            textifiedTriples = pickle.load(f)\n",
        "        with open (files_path + 'targetSentences_dev_1.txt', 'rb') as f:\n",
        "            sentences = pickle.load(f)\n",
        "        '''\n",
        "        notParenthesesIndexes = []\n",
        "        for i, triple in enumerate(textifiedTriples):\n",
        "            if not '(' in triple:\n",
        "                notParenthesesIndexes.append(i)\n",
        "        '''\n",
        "    else:\n",
        "        if chooseTest == 'exchangedObjSubTriples':\n",
        "            results_path = exchangedObjSubTriplesResults_path\n",
        "            with open (files_path + 'textified_exchangedObjSubTriples.txt', 'rb') as f:\n",
        "                textifiedTriples = pickle.load(f)\n",
        "\n",
        "        elif chooseTest == 'randomPropertyTriples':\n",
        "            results_path = randomPropertyTriplesResults_path\n",
        "            with open (files_path + 'textified_randomPropertyTriples.txt', 'rb') as f:\n",
        "                textifiedTriples = pickle.load(f)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"chooseTest must be one of the following: 'originalTriples', 'exchangedObjSubTriples', 'randomPropertyTriples'\")\n",
        "\n",
        "\n",
        "    with open (files_path + 'triples_dev_1_split.txt', 'rb') as f:\n",
        "        triples = pickle.load(f)\n",
        "    properties_set = get_allProperties(triples)\n",
        "    triples_properties = [triple[1] for triple in triples]\n",
        "    properties_errors = {property: 0 for property in properties_set}\n",
        "\n",
        "    '''with open (files_path + 'triples_categories.txt', 'rb') as f:\n",
        "        triples_categories = pickle.load(f)'''\n",
        "\n",
        "    highestScores = []\n",
        "    # offTheShelf_highestScores = []\n",
        "\n",
        "    '''categories_set = set(triples_categories)\n",
        "    categories_errors = {category: 0 for category in categories_set}'''\n",
        "\n",
        "    classifiedResults_sum = [[0]*4 for i in range(len(thresholds))]   #[truePositives, trueNegatives, falsePositives, falseNegatives] for each threshold\n",
        "    incorrectTops_sum = 0\n",
        "    incorrectTop1_sum = 0\n",
        "    correctSentences_means = []\n",
        "    differences = []\n",
        "\n",
        "    fineTuned_errors = []\n",
        "\n",
        "    #for i in tqdm(notParenthesesIndexes):\n",
        "    for i, textifiedTriple in enumerate(tqdm(textifiedTriples)):\n",
        "    #for i in tqdm(range(textifiedTriples)):\n",
        "\n",
        "        #textifiedTriple = textifiedTriples[i]\n",
        "        correctSentences = sentences[i] if chooseTest == 'originalTriples' else None\n",
        "\n",
        "        results1 = ''\n",
        "        results2 = ''\n",
        "        if model == 'fineTuned':\n",
        "            results1_name = 'triple'+str(i)+'_results1.txt'\n",
        "            pickle_off = open(results_path + results1_name, 'rb')\n",
        "            results1 = pickle.load(pickle_off)    \n",
        "            highestScores.append(results1[0][1])\n",
        "            for j, threshold in enumerate(thresholds):\n",
        "                fineTuned_classifiedResults = classify_results(results1, threshold, correctSentences, chooseTest)\n",
        "                classifiedResults_sum[j] = [i+j for i,j in zip(classifiedResults_sum[j], fineTuned_classifiedResults)]\n",
        "\n",
        "        elif model == 'offTheShelf':\n",
        "            results2_name = 'triple'+str(i)+'_results2.txt'\n",
        "            pickle_off = open(results_path + results2_name, 'rb')\n",
        "            results2 = pickle.load(pickle_off)\n",
        "            highestScores.append(results2[0][1])\n",
        "            for j, threshold in enumerate(thresholds):\n",
        "                offTheShelf_classifiedResults = classify_results(results2, threshold, correctSentences, chooseTest)\n",
        "                classifiedResults_sum[j] = [i+j for i,j in zip(classifiedResults_sum[j], offTheShelf_classifiedResults)]\n",
        "\n",
        "\n",
        "        metrics = get_allClassificationMetrics(thresholds, classifiedResults_sum)\n",
        "\n",
        "        model_f1List = [[threshold, metrics[i][3]] for i, threshold in enumerate(thresholds)]\n",
        "        model_f1Top = sorted(model_f1List, key=lambda x: x[1], reverse=True)\n",
        "        # offTheShelf_f1List = [[threshold, offTheShelf_metrics[i][3]] for i, threshold in enumerate(thresholds)]\n",
        "        # offTheShelf_f1Top = sorted(offTheShelf_f1List, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        if chooseTest == 'originalTriples':\n",
        "            topIsCorrect = ''\n",
        "            if model == 'fineTuned':\n",
        "                topIsCorrect = top_isCorrect(results1, correctSentences)\n",
        "                correctSentencesMean = get_correctSentencesMean(results1, correctSentences, topIsCorrect)\n",
        "                if results1[0][0] not in correctSentences:\n",
        "                   incorrectTop1_sum += 1\n",
        "            elif model == 'offTheShelf':\n",
        "                topIsCorrect = top_isCorrect(results2, correctSentences)\n",
        "                correctSentencesMean = get_correctSentencesMean(results2, correctSentences, topIsCorrect)\n",
        "                if results2[0][0] not in correctSentences:\n",
        "                    incorrectTop1_sum += 1\n",
        "            \n",
        "            correctSentences_means.append(correctSentencesMean)\n",
        "\n",
        "            difference = ''\n",
        "\n",
        "            if topIsCorrect: \n",
        "                if model == 'fineTuned':\n",
        "                    if len(correctSentences) < len(results1):\n",
        "                        difference = correctSentencesMean - results1[len(correctSentences)][1]\n",
        "                    else:\n",
        "                        difference = 0.212 ##\n",
        "                elif model == 'offTheShelf':\n",
        "                    if len(correctSentences) < len(results2):\n",
        "                        difference = correctSentencesMean - results2[len(correctSentences)][1]\n",
        "                    else:\n",
        "                        difference = 0.110\n",
        "            else:\n",
        "                incorrectTops_sum += 1\n",
        "                difference = None\n",
        "                if model == 'fineTuned':\n",
        "                   fineTuned_errors.append([textifiedTriple, correctSentences, results1[:5]])\n",
        "                '''categories_errors[triples_categories[i]] += 1'''\n",
        "                properties_errors[triples_properties[i]] += 1\n",
        "\n",
        "            if model == 'fineTuned':\n",
        "                correctSentences_means.append(get_correctSentencesMean(results1, correctSentences, topIsCorrect))\n",
        "            elif model == 'offTheShelf':\n",
        "                correctSentences_means.append(get_correctSentencesMean(results2, correctSentences, topIsCorrect))\n",
        "\n",
        "            differences.append(difference)\n",
        "\n",
        "    highestScores_mean = statistics.mean(highestScores)\n",
        "\n",
        "    if chooseTest == 'originalTriples':\n",
        "        '''category_errors = [[category, categories_errors[category]] for category in categories_errors]'''\n",
        "        property_errors = [[property, properties_errors[property]] for property in properties_errors]\n",
        "\n",
        "        incorrectTops_percentage = incorrectTops_sum / n * 100\n",
        "\n",
        "        incorrectTop1_percentage = incorrectTop1_sum / n * 100\n",
        "\n",
        "        correctSentencesMeans_mean = statistics.mean(correctSentences_means)\n",
        "\n",
        "        differences_mean = statistics.mean([difference for difference in filter(None, differences)])\n",
        "\n",
        "        results = [metrics,\n",
        "                model_f1Top,\n",
        "                classifiedResults_sum,\n",
        "                incorrectTops_percentage,\n",
        "                incorrectTop1_percentage,\n",
        "                correctSentencesMeans_mean,\n",
        "                differences_mean,\n",
        "                highestScores_mean,\n",
        "                fineTuned_errors, thresholds, property_errors]\n",
        "    else:\n",
        "        results = [metrics,\n",
        "                model_f1Top,\n",
        "                classifiedResults_sum,\n",
        "                highestScores_mean,\n",
        "                thresholds]\n",
        "\n",
        "    return results\n",
        "\n",
        "def print_infoResults(results, chooseTest, model):\n",
        "    if chooseTest == 'originalTriples':\n",
        "        [metrics,\n",
        "        model_f1Top,\n",
        "        classifiedResults_sum,\n",
        "        incorrectTops_percentage,\n",
        "        incorrectTop1_percentage,\n",
        "        correctSentencesMeans_mean,\n",
        "        differences_mean,\n",
        "        highestScores_mean,\n",
        "        fineTuned_errors, thresholds, property_errors] = results\n",
        "    else:\n",
        "         [metrics,\n",
        "          model_f1Top,\n",
        "          classifiedResults_sum,\n",
        "          highestScores_mean,\n",
        "          thresholds] = results\n",
        "\n",
        "    print('CLASSIFICATION METRICS')\n",
        "    for i, threshold in enumerate(thresholds):\n",
        "        print(threshold)\n",
        "        print('\\tEvaluated model:  ')\n",
        "        print('\\t    [True Positives, False Positives]: [{:.0f}, {:.0f}]'.format(classifiedResults_sum[i][0], classifiedResults_sum[i][1]))\n",
        "        print('\\t    [False Negatives, True Negatives]: [{:.0f}, {:.0f}]'.format(classifiedResults_sum[i][3], classifiedResults_sum[i][2]))\n",
        "        print('\\t\\tPrecision: {:.3f}'.format(metrics[i][0]))\n",
        "        print('\\t\\tRecall: {:.3f}'.format(metrics[i][1]))\n",
        "        print('\\t\\taccuracy: {:.3f}'.format(metrics[i][2]))\n",
        "        print('\\t\\tf1-score: {:.3f}'.format(metrics[i][3]))\n",
        "        print()\n",
        "    print('TOP F1 THRESHOLDS')\n",
        "    print('\\tEvaluated model:')\n",
        "    print('\\t\\t', model_f1Top[:5])\n",
        "    print()\n",
        "    print('HIGHEST SCORES MEAN')\n",
        "    print('Evaluated model:  {:.5f}'.format(highestScores_mean))\n",
        "    print()\n",
        "    \n",
        "    if chooseTest == 'originalTriples':\n",
        "        print('NOT CORRECT TOPS')\n",
        "        print('Evaluated model:  {:.2f}%'.format(incorrectTops_percentage))\n",
        "        print()\n",
        "        print('NOT CORRECT TOP1')\n",
        "        print('Evaluated model:  {:.2f}%'.format(incorrectTop1_percentage))\n",
        "        print()\n",
        "        print('CORRECT SENTENCES SCORE MEANS MEAN')\n",
        "        print('Evaluated model: ', '{:.3f}'.format(correctSentencesMeans_mean))\n",
        "        print()\n",
        "        print('DIFFERENCE BETWEEN TOP SCORE MEANS AND FIRST NON CORRECT MEAN')\n",
        "        print('Evaluated model: ', '{:.3f}'.format(differences_mean))\n",
        "        print()\n",
        "        if model == 'fineTuned':\n",
        "            print('FINE-TUNED MODEL ERRORS')\n",
        "            for i, [textified_triple, correct_sentences, results] in enumerate(fineTuned_errors):\n",
        "                print(i+1, '---', textified_triple) #, '---  (category: '+category+')')\n",
        "                print('\\tCorrect Sentences')\n",
        "                for j, sentence in enumerate(correct_sentences):\n",
        "                    print('\\t\\t', j+1, sentence)\n",
        "                print('\\tTop sentences')\n",
        "                for j, result in enumerate(results):\n",
        "                    print('\\t\\t', j+1, result[0], '=>', result[1])\n",
        "                print()\n",
        "            print()\n",
        "        '''print('CATEGORY ERRORS')\n",
        "        for [category, sum] in category_errors:\n",
        "            print('\\t', category+':', sum)\n",
        "        print()'''\n",
        "        # print('PROPERTY ERRORS')\n",
        "        # for [property, sum] in property_errors:\n",
        "        #     print('\\t', property+':', sum)\n",
        "        # print()"
      ],
      "metadata": {
        "id": "5D62dsotceue"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EV-3.2 Run (INPUT NEEDED: model to evaluate)"
      ],
      "metadata": {
        "id": "dDO1kWaU5ybp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Pick one model type to evaluate\n",
        "model = 'offTheShelf'#@param ['offTheShelf', 'fineTuned']\n",
        "#'originalTriples', 'exchangedObjSubTriples', 'randomPropertyTriples'\n",
        "results = checkResults(thresholds, 401, files_path, 'originalTriples', model)\n",
        "print_infoResults(results, 'originalTriples', model)"
      ],
      "metadata": {
        "id": "hpHFpQ4acqz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FT FINE TUNING"
      ],
      "metadata": {
        "id": "9QCwCiBMrQxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FT-1 Dataset**"
      ],
      "metadata": {
        "id": "Y1sMs-r5I2HT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FT-1.1 Load WebNLG v3.0"
      ],
      "metadata": {
        "id": "wnTP6tqf1Oe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For clearing outputs of installs\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# datasets is for loading datasets from HuggingFace\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "clear_output()\n",
        "\n",
        "webnlg = load_dataset('web_nlg', 'release_v3.0_en')"
      ],
      "metadata": {
        "id": "xc0Zsb89VMq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load triples of size 1 from the WebNLG training data"
      ],
      "metadata": {
        "id": "3WzSE4YLYwB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_webnlg(webnlg):\n",
        "    dataset = []\n",
        "    for sample in webnlg['train']:\n",
        "        if sample['size'] == 1:\n",
        "            dataset.append([sample['modified_triple_sets']['mtriple_set'][0][0], sample['lex']['text']])\n",
        "    return dataset\n",
        "\n",
        "dataset = load_webnlg(webnlg)"
      ],
      "metadata": {
        "id": "5i4e5Gcf1VNI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge repeated triples"
      ],
      "metadata": {
        "id": "J9fDcpJdYuSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_repeated_triples(dataset):\n",
        "\n",
        "    triples = [] #create a list of triples to then find the repeated ones\n",
        "    for sample in dataset:\n",
        "        triples.append(sample[0])\n",
        "\n",
        "    duplicates = [triple for triple in triples if triples.count(triple) > 1] #duplicated triples list\n",
        "    \n",
        "    to_delete = []\n",
        "    for i, sample in enumerate(dataset): #Append sentences from repeated triples in the first occurrence triple list \n",
        "        triple = sample[0]\n",
        "        if triple in duplicates:\n",
        "            first_occurrence = triples.index(triple)\n",
        "            if i != first_occurrence:\n",
        "                for sentence in sample[1]:\n",
        "                    dataset[first_occurrence][1].append(sentence)\n",
        "                to_delete.append(i) #Save indexes of triples to delete\n",
        "\n",
        "    for i, index in enumerate(to_delete): #Delete repeated triples\n",
        "        del dataset[index - i]\n",
        "\n",
        "    for sample in dataset: #Remove repeated sentences after merging\n",
        "        sentence_set = set(sample[1])\n",
        "        sample[1] = list(sentence_set)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "dataset = merge_repeated_triples(dataset)"
      ],
      "metadata": {
        "id": "IJYgIflGxx0w"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[0])\n",
        "print(len(dataset))"
      ],
      "metadata": {
        "id": "zPVK9MtlSghO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c759dbe-6902-4a2d-d059-e16ca0f29b40"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Aarhus_Airport | cityServed | \"Aarhus, Denmark\"', ['Aarhus Airport serves the city of Aarhus, Denmark.', 'The Aarhus is the airport of Aarhus, Denmark.']]\n",
            "3107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text triples: Each triple is a string in this form: \"Subject | Property | Object\""
      ],
      "metadata": {
        "id": "2sMPKHS1e1B1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_triples(dataset):\n",
        "    text_triples = []\n",
        "    for sample in dataset:\n",
        "        text_triples.append(sample[0])\n",
        "    return text_triples\n",
        "\n",
        "text_triples = get_text_triples(dataset)"
      ],
      "metadata": {
        "id": "_iAJ2l9H4h04"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Num text triples:', len(text_triples), '(Expected: 3107)')\n",
        "print()\n",
        "print('text_triples[0]:')\n",
        "text_triples[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "NJm5r_3pfd6Y",
        "outputId": "bfb99c35-76b5-4ba8-9ce4-1f5467575bac"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num text triples: 3107 (Expected: 3107)\n",
            "\n",
            "text_triples[0]:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Aarhus_Airport | cityServed | \"Aarhus, Denmark\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentences list: a list with all sentences corresponding to 1-triple inputs"
      ],
      "metadata": {
        "id": "zlwSl3QFGfdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentences(dataset):\n",
        "    sentences = []\n",
        "    for sample in dataset:\n",
        "        for sentence in sample[1]:\n",
        "            sentences.append(sentence)\n",
        "    return sentences\n",
        "\n",
        "sentences = get_sentences(dataset)"
      ],
      "metadata": {
        "id": "wzL4H8m2EOkI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Num sentences:', len(sentences), '(Expected: 7630)')\n",
        "print()\n",
        "print('sentences[0]:')\n",
        "sentences[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "21yAs7rVfgDw",
        "outputId": "b2f54281-4e03-4623-f1f8-a22c1c4c332a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num sentences: 7630 (Expected: 7630)\n",
            "\n",
            "sentences[0]:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Aarhus Airport serves the city of Aarhus, Denmark.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Triples list: triples are stored as a list of 3 elements to access Subj, Property and Obj separately"
      ],
      "metadata": {
        "id": "Q464VuorGixO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_triples(text_triples):\n",
        "    triples = []\n",
        "    for text_triple in text_triples:\n",
        "        firstBarIndex = text_triple.find('|')-1\n",
        "        secondBarIndex = text_triple.rfind('|')+2\n",
        "\n",
        "        sub = text_triple[:firstBarIndex]\n",
        "        prop = text_triple[firstBarIndex + 3 : secondBarIndex-3]\n",
        "        obj = text_triple[secondBarIndex:]\n",
        "\n",
        "        triple = [sub, prop, obj]\n",
        "        triples.append(triple)\n",
        "    return triples\n",
        "\n",
        "triples = get_triples(text_triples)"
      ],
      "metadata": {
        "id": "Jh46C-C99Z2g"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Num triples:', len(triples), '(Expected: 3107)')\n",
        "print()\n",
        "print('triples[0]:')\n",
        "triples[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZN9yAL9fkOX",
        "outputId": "a6d7e1e5-db39-4fe5-a955-9bff13b9071a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num triples: 3107 (Expected: 3107)\n",
            "\n",
            "triples[0]:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Aarhus_Airport', 'cityServed', '\"Aarhus, Denmark\"']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save data\n",
        "# import pickle\n",
        "\n",
        "# fineTuningDataset_path = '/content/' #Write path to save the dataset\n",
        "# with open(fineTuningDataset_path + 'triples_split.txt', 'wb') as fh:\n",
        "#    pickle.dump(triples, fh)\n",
        "\n",
        "# with open(fineTuningDataset_path + 'triples_raw.txt', 'wb') as fh:\n",
        "#    pickle.dump(text_triples, fh)"
      ],
      "metadata": {
        "id": "DNEsR1EPP7_z"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FT-1.2 Extract info needed for Finetuning dataset"
      ],
      "metadata": {
        "id": "OhTEyiIxGlp9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct intermediate dataset: eventually, we want a dataset of sentence-like pairs with approximate similarity score between them."
      ],
      "metadata": {
        "id": "FuzgWOj45q7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build a list of 2 dictionaries to store sentences with 2 different levels of similarity with the input triple\n",
        "intermediate_dataset = [dict(), dict()]\n",
        "for category in intermediate_dataset:\n",
        "    for text_triple in text_triples:\n",
        "        category[text_triple] = set()"
      ],
      "metadata": {
        "id": "LTM7UHzpwS_3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(intermediate_dataset[0]))\n",
        "print(intermediate_dataset[0]['Aarhus_Airport | cityServed | \"Aarhus, Denmark\"'])"
      ],
      "metadata": {
        "id": "hY-wnl82USA-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b47027-583d-44b0-c910-bf78c6bc6ede"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3107\n",
            "set()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For each triple, compare it to all other triples and get a set with the intersection of the two triples.\n",
        "for i, triple1 in enumerate(triples):\n",
        "    triple_1 = set(triple1)\n",
        "    for j, triple2 in enumerate(triples):\n",
        "        triple_2 = set(triple2)\n",
        "        final = triple_1 & triple_2\n",
        "        \n",
        "        # Group together the sentences that verbalise triples that have all or no elements (Subj, Prop, Obj) in common with triple1\n",
        "        if len(final) == 0:\n",
        "            for sentence in dataset[j][1]:\n",
        "                intermediate_dataset[0][text_triples[i]].add(sentence)\n",
        "        elif len(final) == 3:\n",
        "            for sentence in dataset[j][1]:\n",
        "                intermediate_dataset[1][text_triples[i]].add(sentence)\n",
        "    # print(final)\n"
      ],
      "metadata": {
        "id": "uX9r5dmK5y4E"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(intermediate_dataset[0]))\n",
        "print(list(intermediate_dataset[0]['Aarhus_Airport | cityServed | \"Aarhus, Denmark\"'])[:10])\n",
        "print(list(intermediate_dataset[1]['Aarhus_Airport | cityServed | \"Aarhus, Denmark\"'])[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upYe1J4yyA5a",
        "outputId": "f95b7cac-1e9d-4432-e6b7-34f1421d0067"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3107\n",
            "['American was the nationality of John Buscema.', 'Atlantic City, New Jersey is in the United States.', 'Atlanta is the capital of Georgia, United States.', 'John Buscema has won the Inkpot Award.', 'Robert E. Lee was a commander at the Battle of Fredericksburg.', '05L/23R is the runway name at Angola International airport.', 'Allen Forrest was born in Fort Campbell.', '11 Diagonal Street has a floor area of 1200 square metres.', 'The dish bhajji is related to pakora.', 'The novel A Long Long Way was followed by The Secret Scripture.']\n",
            "['Aarhus Airport serves the city of Aarhus, Denmark.', 'The Aarhus is the airport of Aarhus, Denmark.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save dataset"
      ],
      "metadata": {
        "id": "3zsjMmqa3Ezd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "fineTuningDataset_path = '/content/' #Write path to save the dataset\n",
        "with open(fineTuningDataset_path + 'intermediate_dataset.txt', 'wb') as fh:\n",
        "   pickle.dump(intermediate_dataset, fh)"
      ],
      "metadata": {
        "id": "IjUFTGhr3qt_"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FT-1.3 Create text/string versions of the triples to be compared to the sentences"
      ],
      "metadata": {
        "id": "XWfoXcZlKppG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(triples[0])\n",
        "\n",
        "def camelCaseClean(text):\n",
        "  words = [[text[0]]]\n",
        "  for c in text[1:]:\n",
        "    if words[-1][-1].islower() and c.isupper():\n",
        "      words.append(list(c.lower()))\n",
        "    else:\n",
        "      words[-1].append(c)\n",
        "  words = [''.join(word) for word in words]\n",
        "  cleaned = ' '.join(words)\n",
        "  return cleaned\n",
        "\n",
        "textified_triples_noTypes_noTags = []\n",
        "\n",
        "for triple in triples:\n",
        "  # remove underscores and quotes \n",
        "  subj = triple[0].replace('_',' ').replace('\"','')\n",
        "  obj = triple[2].replace('_',' ').replace('\"','')\n",
        "  # split property names\n",
        "  prop = camelCaseClean(triple[1])\n",
        "  \n",
        "  text = subj+' '+prop+' '+obj+' .'\n",
        "  textified_triples_noTypes_noTags.append(text)\n",
        "\n",
        "# print(textified_triples_noTypes_noTags[0])\n",
        "# print(len(textified_triples_noTypes_noTags))"
      ],
      "metadata": {
        "id": "17k0SGp2aLY3"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "fineTuningDataset_path = '/content/' #Write path to save the dataset\n",
        "with open(fineTuningDataset_path + 'triples_train_1_textified.txt', 'wb') as fh:\n",
        "   pickle.dump(textified_triples_noTypes_noTags, fh)"
      ],
      "metadata": {
        "id": "WnwmuJHCbG5G"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FT-1.4 Format dataset for Sentence Transformers."
      ],
      "metadata": {
        "id": "78zTXhYt8A53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the files produced in the previous step"
      ],
      "metadata": {
        "id": "vSUhDeJ52NWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "!pip install -U sentence-transformers\n",
        "\n",
        "clear_output()\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Load created dataset, with triples and scores\n",
        "fineTuningDataset_path = '/content/' #Write path to save the dataset\n",
        "pickle_off = open(fineTuningDataset_path + 'intermediate_dataset.txt', 'rb')\n",
        "interm_dataset = pickle.load(pickle_off)\n",
        "\n",
        "# Load textified triples\n",
        "pickle_off = open(fineTuningDataset_path + 'triples_train_1_textified.txt', 'rb')\n",
        "textified_triples = pickle.load(pickle_off)"
      ],
      "metadata": {
        "id": "L6d2vlGr4WHm"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print some examples\n",
        "sample_triples = ['Aarhus_Airport | cityServed | \"Aarhus, Denmark\"', 'Aarhus_Airport | cityServed | Aarhus', 'Christian_Burns | genre | House_music', 'Athens_International_Airport | runwayLength | 3800.0', 'New_York_City | country | United_States']\n",
        "\n",
        "for triple in sample_triples:\n",
        "  print(triple)\n",
        "  print(list(interm_dataset[1][triple])[:10])\n",
        "  print(list(interm_dataset[0][triple])[:10])\n",
        "  print()\n",
        "\n",
        "print(len(textified_triples))\n",
        "print(textified_triples[0])"
      ],
      "metadata": {
        "id": "JShphJNPkqE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df131d6-222c-46e3-91ae-dfddbd4f10cf"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aarhus_Airport | cityServed | \"Aarhus, Denmark\"\n",
            "['Aarhus Airport serves the city of Aarhus, Denmark.', 'The Aarhus is the airport of Aarhus, Denmark.']\n",
            "['American was the nationality of John Buscema.', 'Atlanta is the capital of Georgia, United States.', 'Atlantic City, New Jersey is in the United States.', 'John Buscema has won the Inkpot Award.', 'Robert E. Lee was a commander at the Battle of Fredericksburg.', '05L/23R is the runway name at Angola International airport.', 'Allen Forrest was born in Fort Campbell.', '11 Diagonal Street has a floor area of 1200 square metres.', 'The dish bhajji is related to pakora.', 'The novel A Long Long Way was followed by The Secret Scripture.']\n",
            "\n",
            "Aarhus_Airport | cityServed | Aarhus\n",
            "['Aarhus airport serves the city of Aarhus.']\n",
            "['American was the nationality of John Buscema.', 'Atlanta is the capital of Georgia, United States.', 'Atlantic City, New Jersey is in the United States.', 'John Buscema has won the Inkpot Award.', 'Robert E. Lee was a commander at the Battle of Fredericksburg.', '05L/23R is the runway name at Angola International airport.', 'Allen Forrest was born in Fort Campbell.', '11 Diagonal Street has a floor area of 1200 square metres.', 'The dish bhajji is related to pakora.', 'The novel A Long Long Way was followed by The Secret Scripture.']\n",
            "\n",
            "Christian_Burns | genre | House_music\n",
            "['Christian Burns performs house music.', 'The musical genre of Christian Burns is house music.', 'Christian Burns is an exponent of House music.']\n",
            "['American was the nationality of John Buscema.', 'Atlanta is the capital of Georgia, United States.', 'Atlantic City, New Jersey is in the United States.', 'John Buscema has won the Inkpot Award.', 'Robert E. Lee was a commander at the Battle of Fredericksburg.', '05L/23R is the runway name at Angola International airport.', 'Allen Forrest was born in Fort Campbell.', '11 Diagonal Street has a floor area of 1200 square metres.', 'The dish bhajji is related to pakora.', 'The novel A Long Long Way was followed by The Secret Scripture.']\n",
            "\n",
            "Athens_International_Airport | runwayLength | 3800.0\n",
            "['The runway length at Athens International Airport is 3,800.', 'The runway length of Athens International Airport is 3800.0.']\n",
            "['American was the nationality of John Buscema.', 'Atlanta is the capital of Georgia, United States.', 'Atlantic City, New Jersey is in the United States.', 'John Buscema has won the Inkpot Award.', 'Robert E. Lee was a commander at the Battle of Fredericksburg.', '05L/23R is the runway name at Angola International airport.', 'Allen Forrest was born in Fort Campbell.', '11 Diagonal Street has a floor area of 1200 square metres.', 'The dish bhajji is related to pakora.', 'The novel A Long Long Way was followed by The Secret Scripture.']\n",
            "\n",
            "New_York_City | country | United_States\n",
            "['New York city is located in the U.S.', 'New York City is located in the United States.', 'New York City is in the United States.']\n",
            "['American was the nationality of John Buscema.', 'John Buscema has won the Inkpot Award.', 'Atlanta is the capital of Georgia, United States.', 'Robert E. Lee was a commander at the Battle of Fredericksburg.', '05L/23R is the runway name at Angola International airport.', 'Allen Forrest was born in Fort Campbell.', '11 Diagonal Street has a floor area of 1200 square metres.', 'The dish bhajji is related to pakora.', 'The novel A Long Long Way was followed by The Secret Scripture.', 'The governing body of the Eastern Province Sri Lanka is the Eastern Provincial Council.']\n",
            "\n",
            "3107\n",
            "Aarhus Airport city served Aarhus, Denmark .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final dataset is a list of objects with two attributes, \"texts\", which is a list of two sentences (texts=[sentence1, sentence2]), and \"label\", which is the similarity score between these two senences (label=score)."
      ],
      "metadata": {
        "id": "_m6FrVlMHC6G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ec5yjbssPX3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "bd0d668a-44b4-45a7-beaa-061cb131c4ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Category 0: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 3107/3107 [01:41<00:00, 30.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Category 1: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 3107/3107 [00:00<00:00, 175452.07it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from sentence_transformers import InputExample\n",
        "\n",
        "triples = list(interm_dataset[0].keys())\n",
        "# triples[999], textified_triples[999]\n",
        "\n",
        "ft_sentencePair_score_list = []\n",
        "#visualize_examples = []\n",
        "for i, category in enumerate(interm_dataset):\n",
        "    score = float(i)\n",
        "    print('Category', i, end=': ')\n",
        "    for j, triple in enumerate(tqdm(triples)):\n",
        "        for sentence in category[triple]:\n",
        "            sentence1 = textified_triples[j]\n",
        "            sentence2 = sentence\n",
        "            inp_example = InputExample(texts=[sentence1, sentence2], label=score)\n",
        "            #visualize_example = [[sentence1, sentence2], score]\n",
        "            ft_sentencePair_score_list.append(inp_example)\n",
        "            #visualize_examples.append(visualize_example)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLNou-orz9BR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b924733a-5258-43fb-ff36-a66d16372474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "<InputExample> label: 0.0, texts: Aarhus Airport city served Aarhus, Denmark .; Olive oil is an ingredient in Arrabbiata sauce.\n"
          ]
        }
      ],
      "source": [
        "print(len(ft_sentencePair_score_list) == 23252141 + 7645)\n",
        "print(ft_sentencePair_score_list[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Balance the dataset so as to have as many examples of positive and negative data."
      ],
      "metadata": {
        "id": "ynoSnanfi4gg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iLe5wvs_dc7"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "category0 = ft_sentencePair_score_list[:23252141] # [:23252141]\n",
        "category1 = ft_sentencePair_score_list[23252141:]\n",
        "\n",
        "shuffled_category0 = random.sample(category0, len(category0))\n",
        "\n",
        "category0 = shuffled_category0[:7645]\n",
        "\n",
        "ft_sentencePair_score_list = category0 + category1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(ft_sentencePair_score_list))\n",
        "# x = 0\n",
        "# while x < 100:\n",
        "#   print(str(ft_sentencePair_score_list[x].label)+': '+str(ft_sentencePair_score_list[x].texts))\n",
        "#   x += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoV-GOr05KTr",
        "outputId": "1b33b9b6-84bb-46ad-d48d-8b39caca85c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15290\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRtV6VneFUC_"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(fineTuningDataset_path + 'fine_tuning_dataset.txt', 'wb') as fh:\n",
        "   pickle.dump(ft_sentencePair_score_list, fh)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load created dataset if you want to have a look at it"
      ],
      "metadata": {
        "id": "IEzVSLiZ099K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "fineTuningDataset_path = '/content/'\n",
        "pickle_off = open(fineTuningDataset_path + 'fine_tuning_dataset.txt', 'rb')\n",
        "new_dataset = pickle.load(pickle_off)"
      ],
      "metadata": {
        "id": "SyEhR-MXzztq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(new_dataset[7644].texts)\n",
        "print(new_dataset[7644].label)\n",
        "print(new_dataset[7645].texts)\n",
        "print(new_dataset[7645].label)"
      ],
      "metadata": {
        "id": "5l_5NOXW0A7v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a13f62db-1552-4915-803a-ccb3c3c3fa95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Appleton International Airport city served Appleton, Wisconsin .', 'The powertype of the Aleksey Chirikow icebreaker is Wrtsil.']\n",
            "0.0\n",
            "['Aarhus Airport city served Aarhus, Denmark .', 'Aarhus Airport serves the city of Aarhus, Denmark.']\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FT-2 Fine-tuning**"
      ],
      "metadata": {
        "id": "KdZ5qi0WI5hS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FT-2.1 Set parameters, load model (INPUT NEEDED: path to save fine-tuned model)"
      ],
      "metadata": {
        "id": "CbAj0uxMJVDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, util, InputExample\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import os\n",
        "import gzip\n",
        "import csv\n",
        "\n",
        "#### Just some code to print debug information to stdout\n",
        "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
        "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
        "                    level=logging.INFO,\n",
        "                    handlers=[LoggingHandler()])\n",
        "#### /print debug information to stdout\n",
        "\n",
        "#Check if dataset exsist. If not, download and extract  it\n",
        "'''sts_dataset_path = 'datasets/stsbenchmark.tsv.gz'\n",
        "\n",
        "if not os.path.exists(sts_dataset_path):\n",
        "    util.http_get('https://sbert.net/datasets/stsbenchmark.tsv.gz', sts_dataset_path)\n",
        "'''\n",
        "\n",
        "# Read the dataset\n",
        "model_name = 'nli-distilroberta-base-v2'\n",
        "train_batch_size = 16\n",
        "num_epochs = 4\n",
        "model_save_path = '/content/drive/MyDrive/Colab-dump/Lab_Week10/MyModel-'+model_name+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") #finetuned model path and name\n",
        "\n",
        "# Load a pre-trained sentence transformer model\n",
        "model = SentenceTransformer(model_name)"
      ],
      "metadata": {
        "id": "Jw0yIMMeI9TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FT-2.2 Load dataset and create splits"
      ],
      "metadata": {
        "id": "VF2q0Tv2u7EI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "fineTuningDataset_path = '/content/'\n",
        "pickle_off = open(fineTuningDataset_path + 'fine_tuning_dataset.txt', 'rb')\n",
        "input_examples = pickle.load(pickle_off)"
      ],
      "metadata": {
        "id": "MJv6bqHZJn9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "shuffled_input_examples = random.sample(input_examples, len(input_examples)) # shuffles the ordering of filenames (deterministic given the chosen seed)\n",
        "\n",
        "split_1 = int(0.70 * len(shuffled_input_examples))\n",
        "split_2 = int(0.85 * len(shuffled_input_examples))\n",
        "\n",
        "train_samples = shuffled_input_examples[:split_1]\n",
        "dev_samples = shuffled_input_examples[split_1:split_2]\n",
        "test_samples = shuffled_input_examples[split_2:]"
      ],
      "metadata": {
        "id": "XziL6jBlJ9SK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_samples), len(dev_samples), len(test_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3ZmXqlVKAYa",
        "outputId": "3e9f11a7-3c9a-4a1d-ab02-1280e379af17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10703, 2293, 2294)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FT-2.3 Create model (use GPU)"
      ],
      "metadata": {
        "id": "QMTEWC4_vE2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n",
        "train_loss = losses.CosineSimilarityLoss(model=model)\n",
        "\n",
        "# Development set: Measure correlation between cosine score and gold labels\n",
        "logging.info(\"Read STSbenchmark dev dataset\")\n",
        "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, name='sts-dev')\n",
        "\n",
        "# Configure the training. We skip evaluation in this example\n",
        "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n",
        "logging.info(\"Warmup-steps: {}\".format(warmup_steps))"
      ],
      "metadata": {
        "id": "OwovriXfKCY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model (Use GPU)\n",
        "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
        "          evaluator=evaluator,\n",
        "          epochs=num_epochs,\n",
        "          evaluation_steps=1000,\n",
        "          warmup_steps=warmup_steps,\n",
        "          output_path=model_save_path)"
      ],
      "metadata": {
        "id": "tk9qmyIIKEcU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}