{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cKkbKxI1EnUm"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### EV- 0.0 Test sentence similarity models (optional)\n",
        "Just to play with a couple of models and a few sentences."
      ],
      "metadata": {
        "id": "sGGd1Fr3aOJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Transformers\n",
        "from IPython.display import clear_output\n",
        "! pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "iq3LTFI1Y9L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "model = SentenceTransformer('nli-distilroberta-base-v2')\n",
        "\n",
        "clear_output()\n",
        "\n",
        "# Two lists of sentences\n",
        "sentences1 = ['The cat sits outside',\n",
        "             'A man is playing guitar',\n",
        "             'The new movie is awesome',\n",
        "              # Triples\n",
        "              # 'Aarhus_Airport | cityServed | Aarhus,_Denmark',\n",
        "              # 'Aarhus_Airport | location | Tirstrup',\n",
        "              # \"Textified\" triples\n",
        "              # 'Aarhus Airport city served Aarhus, Denmark',\n",
        "              # 'Aarhus Airport location Tirstrup'\n",
        "              ]\n",
        "\n",
        "sentences2 = ['The dog plays in the garden',\n",
        "              'A woman watches TV',\n",
        "              'The new movie is so great',\n",
        "              # 'Aarhus Airport serves the city of Aarhus, Denmark',\n",
        "              # 'Aarhus Airport is in Tirstrup',\n",
        "              # 'Aarhus Airport serves the city of Aarhus, Denmark',\n",
        "              # 'Aarhus Airport is in Tirstrup'\n",
        "              ]\n",
        "\n",
        "#Compute embedding for both lists\n",
        "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
        "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
        "\n",
        "#Compute cosine-similarities\n",
        "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
        "\n",
        "#Output the pairs with their score\n",
        "for i in range(len(sentences1)):\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hK9V6jXh1k8k",
        "outputId": "e09d1039-2ab5-4a88-e976-bc05d6f0a614"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cat sits outside \t\t The dog plays in the garden \t\t Score: 0.2289\n",
            "A man is playing guitar \t\t A woman watches TV \t\t Score: -0.0171\n",
            "The new movie is awesome \t\t The new movie is so great \t\t Score: 0.9504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**EV-1 Setup**"
      ],
      "metadata": {
        "id": "ZuOt4WNmKUTs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EV-1.1 Mount drive (optional)"
      ],
      "metadata": {
        "id": "UoIlHg0c_Wza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "t_zYDsQRKY0R"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EV-1.2 Download files"
      ],
      "metadata": {
        "id": "Hxdvlz6x_ZyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install --upgrade --no-cache-dir gdown\n",
        "from IPython.display import clear_output\n",
        "! gdown 1M2sxOCSUQJiLt6yC40fGI0QWSeq7jVod\n",
        "! unzip '/content/Lab_Week10.zip'\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "iGUUj86O49Ux"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EV-1.3 Install transformers"
      ],
      "metadata": {
        "id": "aSeWf1n7_faS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install -U sentence-transformers\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "teJtXq-q90bK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####EV-1.4 Load models, set paths (INPUT NEEDED: path to fine-tuned model)"
      ],
      "metadata": {
        "id": "zNq6aTtqAjk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "files_path = '/content/Lab_Week10/Eval/'\n",
        "eval_scores_candidates = '/content/Lab_Week10/Eval/results_candSent/'\n",
        "eval_scores_target = '/content/Lab_Week10/Eval/results_targetSent/'\n",
        "\n",
        "# fineTunedTransformer_path = '/content/drive/MyDrive/Colab-dump/Lab_Week10/MyModel-nli-distilroberta-base-v2-2023-03-22_12-17-11'\n",
        "# fineTuned_model = SentenceTransformer(fineTunedTransformer_path)\n",
        "offTheShelf_model = SentenceTransformer('nli-distilroberta-base-v2')\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "f_Gck0EkAPt7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####EV-1.5 Load files"
      ],
      "metadata": {
        "id": "qswbIJq-B4y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "with open(files_path + 'candidateSentences_dev_1-2.txt', 'rb') as f:\n",
        "  allSentences = list(pickle.load(f))\n",
        "\n",
        "with open(files_path + 'targetSentences_dev_1.txt', 'rb') as f:\n",
        "  referenceSentences = list(pickle.load(f))\n",
        "\n",
        "with open(files_path + 'triples_dev_1_textified.txt', 'rb') as f:\n",
        "  textifiedTriples = pickle.load(f)"
      ],
      "metadata": {
        "id": "ln8qYf0lDE1t"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Expected: 1834, 401, 401')\n",
        "print(len(allSentences), len(textifiedTriples), len(referenceSentences))\n",
        "print(referenceSentences[1], textifiedTriples[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSplnV5q-9rj",
        "outputId": "385a5e20-d900-4811-8496-c54d1fd7a2b2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected: 1834, 401, 401\n",
            "1834 401 401\n",
            "[\"Aarhus Airport's runway length is 2702.0.\", 'The Aarhus Airport has a runway length of 2702.0.'] Aarhus Airport runway length 2702.0 .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **EV-2 Scoring of candidate sentences (use GPU)**\n",
        "Run for fine-tuned model only. "
      ],
      "metadata": {
        "id": "muuIPRoXTS61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####EV-2.1 Scoring Functions"
      ],
      "metadata": {
        "id": "cKkbKxI1EnUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate(textified_triple, sentences_list, model):\n",
        "    to_embed = [[textified_triple], sentences_list]\n",
        "    to_embed = [element for sublist in to_embed for element in sublist]\n",
        "\n",
        "    embeddings = model.encode(to_embed, convert_to_tensor=True)\n",
        "    \n",
        "    results = []\n",
        "\n",
        "    for i in range(1, len(embeddings)):\n",
        "        similarity = float(util.pytorch_cos_sim(embeddings[0], embeddings[i])[0][0])\n",
        "        results.append([sentences_list[i-1], similarity])\n",
        "\n",
        "    results.sort(key=lambda result: result[1], reverse=True)\n",
        "    return results\n",
        "\n",
        "def evaluate_allTriples(textifiedTriples, sentences_list, savePath, model, ft=False):\n",
        "    for i, textified_triple in enumerate(tqdm(textifiedTriples)):\n",
        "        results = evaluate(textified_triple, sentences_list, model)\n",
        "        if ft == False:\n",
        "            with open(savePath + 'triple'+str(i)+'_results2.txt', 'wb') as fh:\n",
        "                pickle.dump(results, fh)\n",
        "        else:\n",
        "            with open(savePath + 'triple'+str(i)+'_results1.txt', 'wb') as fh:\n",
        "                pickle.dump(results, fh)\n",
        "\n",
        "def evaluate_allTriples2(textifiedTriples, referenceSentences, savePath, model, ft=False):\n",
        "    for i, textified_triple in enumerate(tqdm(textifiedTriples)):\n",
        "        results = evaluate(textified_triple, list(referenceSentences[i]), model)\n",
        "        if ft == False:\n",
        "            with open(savePath + 'triple'+str(i)+'_results2.txt', 'wb') as fh:\n",
        "                pickle.dump(results, fh)\n",
        "        else:\n",
        "            with open(savePath + 'triple'+str(i)+'_results1.txt', 'wb') as fh:\n",
        "                pickle.dump(results, fh)"
      ],
      "metadata": {
        "id": "hWUMoLKYEbmt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####EV-2.2 Run scoring"
      ],
      "metadata": {
        "id": "lTmxCDrHF_wL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The results for the off-the-shelf module are already provided, no need to run this cell\n",
        "# evaluate_allTriples(textifiedTriples, allSentences, eval_scores_candidates, offTheShelf_model)\n",
        "# evaluate_allTriples2(textifiedTriples, referenceSentences, eval_scores_target, offTheShelf_model)"
      ],
      "metadata": {
        "id": "sIW3573-GBwr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_allTriples(textifiedTriples, allSentences, eval_scores_candidates, fineTuned_model, ft=True)\n",
        "# evaluate_allTriples2(textifiedTriples, referenceSentences, eval_scores_target, fineTuned_model, ft=True)"
      ],
      "metadata": {
        "id": "eYsPH7fzCdmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EV-2.3 Print raw results"
      ],
      "metadata": {
        "id": "uM9Xyfn7YOwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_result(textified_triple, results, n=None):\n",
        "    print('---', textified_triple, '---')\n",
        "    if not n or n > len(results):\n",
        "        n = len(results)\n",
        "    for i in range(n):\n",
        "   # for i, result in enumerate(results):\n",
        "        print(str(i+1)+'.', 'Similarity: {:.3f}'.format(results[i][1]), '->', results[i][0])\n",
        "\n",
        "def printResults(files_path, model, n):\n",
        "\n",
        "    results_path = eval_scores_candidates\n",
        "    with open (files_path + 'triples_dev_1_textified.txt', 'rb') as f:\n",
        "        textifiedTriples = pickle.load(f)\n",
        "\n",
        "    for i in range(n):\n",
        "        \n",
        "        textified_triple = textifiedTriples[i]\n",
        "        \n",
        "        if model == 'offTheShelf':\n",
        "            result2_name = 'triple'+str(i)+'_results2.txt'\n",
        "            pickle_off = open(results_path + result2_name, 'rb')\n",
        "            result2 = pickle.load(pickle_off)\n",
        "            print('OffTheShelf Model')\n",
        "            print_result(textified_triple, result2, n=10)\n",
        "            print()\n",
        "            print()\n",
        "        \n",
        "        elif model =='fineTuned':\n",
        "            result1_name = 'triple'+str(i)+'_results1.txt'\n",
        "            pickle_off = open(results_path + result1_name, 'rb')\n",
        "            result1 = pickle.load(pickle_off)    \n",
        "            print('FineTuned Model')\n",
        "            print_result(textified_triple, result1, n=10)\n",
        "            print()\n",
        "            print()\n",
        "        \n",
        "        elif model =='both':\n",
        "            result1_name = 'triple'+str(i)+'_results1.txt'\n",
        "            result2_name = 'triple'+str(i)+'_results2.txt'\n",
        "            pickle_off = open(results_path + result1_name, 'rb')\n",
        "            result1 = pickle.load(pickle_off)    \n",
        "            pickle_off = open(results_path + result2_name, 'rb')\n",
        "            result2 = pickle.load(pickle_off)\n",
        "            print('FineTuned Model')\n",
        "            print_result(textified_triple, result1, n=10)\n",
        "            print()\n",
        "            print('OffTheShelf Model')\n",
        "            print_result(textified_triple, result2, n=10)\n",
        "            print()\n",
        "            print()"
      ],
      "metadata": {
        "id": "6VJhGs6aYRpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Second parameter can be 'offTheShelf', 'fineTuned' or 'both'\n",
        "# Third parameter is the number of outputs to print (up to 401)\n",
        "printResults(files_path, 'offTheShelf', 2)"
      ],
      "metadata": {
        "id": "hGyjMnqNYYH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **EV-3 Evaluation results aggregation**\n",
        "Takes the files produced in the scoring phase and extracts global results of the model."
      ],
      "metadata": {
        "id": "Ec3sGTudTkzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EV-3.1 Functions and pre-processing"
      ],
      "metadata": {
        "id": "elYSiCc9cvXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import statistics\n",
        "\n",
        "# To define thresholds to be tested\n",
        "thresholds = []\n",
        "threshold = 0\n",
        "for i in range(99):\n",
        "    threshold += 0.01\n",
        "    thresholds.append(round(threshold, 2))\n",
        "\n",
        "\n",
        "def classify_results(results, threshold, correctSentences, chooseTest):\n",
        "    truePositives = 0\n",
        "    trueNegatives = 0\n",
        "    falsePositives = 0\n",
        "    falseNegatives = 0\n",
        "    for result in results:\n",
        "        if chooseTest == 'originalTriples':\n",
        "            if result[1] >= threshold:\n",
        "                if result[0] in correctSentences:\n",
        "                    truePositives += 1\n",
        "                else:\n",
        "                    falsePositives += 1\n",
        "            else:\n",
        "                if result[0] in correctSentences:\n",
        "                    falseNegatives += 1\n",
        "                else:\n",
        "                    trueNegatives += 1\n",
        "        else:\n",
        "            if result[1] >= threshold:\n",
        "                falsePositives += 1\n",
        "            else:\n",
        "                trueNegatives += 1\n",
        "\n",
        "    classifiedResults = [truePositives, falsePositives, trueNegatives, falseNegatives]\n",
        "\n",
        "    return classifiedResults\n",
        "\n",
        "def top_isCorrect(results, correctSentences):\n",
        "    top_isCorrect = True\n",
        "    top = [result[0] for result in results[:len(correctSentences)]]\n",
        "    for correctSentence in correctSentences:\n",
        "        if correctSentence not in top:\n",
        "            top_isCorrect = False\n",
        "            break\n",
        "    return top_isCorrect\n",
        "\n",
        "def get_correctSentencesMean(results, correctSentences, topIsCorrect):\n",
        "    if topIsCorrect:\n",
        "        correctSentencesMean = statistics.mean([result[1] for result in results[:len(correctSentences)]])\n",
        "    else:\n",
        "        correctSentencesScores = []\n",
        "        for correctSentence in correctSentences:\n",
        "            for result in results:\n",
        "                if result[0] == correctSentence:\n",
        "                    correctSentencesScores.append(result[1])\n",
        "        correctSentencesMean = statistics.mean(correctSentencesScores)\n",
        "\n",
        "    return correctSentencesMean\n",
        "\n",
        "def get_classificationMetrics(classifiedResults_sum, threshold_index):\n",
        "    tp = classifiedResults_sum[threshold_index][0]\n",
        "    fp = classifiedResults_sum[threshold_index][1]\n",
        "    tn = classifiedResults_sum[threshold_index][2]\n",
        "    fn = classifiedResults_sum[threshold_index][3]\n",
        "\n",
        "    if tp == 0:\n",
        "        precision = 0\n",
        "        recall = 0\n",
        "        f1 = 0\n",
        "    else:\n",
        "        precision = tp / (fp + tp)\n",
        "        recall = tp / (fn + tp)\n",
        "        f1 = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    accuracy = (fp + tn) / (tp + fn + tn + fp)\n",
        "\n",
        "    metrics = [precision, recall, accuracy, f1]\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def get_allClassificationMetrics(thresholds, classifiedResults_sum):\n",
        "    \n",
        "    allMetrics = []\n",
        "\n",
        "    for i, threshold in enumerate(thresholds):\n",
        "        metrics = get_classificationMetrics(classifiedResults_sum, i)\n",
        "        allMetrics.append(metrics)\n",
        "\n",
        "    return allMetrics\n",
        "\n",
        "def get_allProperties(triples):\n",
        "    all_properties = set()\n",
        "    for triple in triples:\n",
        "        all_properties.add(triple[1])\n",
        "    return all_properties"
      ],
      "metadata": {
        "id": "PS6PpsUjT41z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open (files_path + 'triples_dev_1_textified.txt', 'rb') as f:\n",
        "#     textifiedTriples = pickle.load(f)\n",
        "\n",
        "# notParenthesesTriples = []\n",
        "# for triple in textifiedTriples:\n",
        "#     if not '(' in triple:\n",
        "#         notParenthesesTriples.append(triple)\n",
        "# print('Expected: 350, 401')\n",
        "# print(len(notParenthesesTriples), len(textifiedTriples))\n",
        "\n",
        "# with open (files_path + 'targetSentences_dev_1.txt', 'rb') as f:\n",
        "#     sentences = pickle.load(f)\n",
        "# print(textifiedTriples[111], sentences[111])"
      ],
      "metadata": {
        "id": "YVsvEGTTba3e"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "originalTriples_path2 = ''\n",
        "\n",
        "def checkResults(thresholds, n, files_path, chooseTest, model, subObjSentences=False):\n",
        "\n",
        "    if chooseTest == 'originalTriples':\n",
        "        if subObjSentences:\n",
        "            results_path = originalTriples_path2\n",
        "        else:\n",
        "            results_path = eval_scores_candidates\n",
        "        with open (files_path + 'triples_dev_1_textified.txt', 'rb') as f:\n",
        "            textifiedTriples = pickle.load(f)\n",
        "        with open (files_path + 'targetSentences_dev_1.txt', 'rb') as f:\n",
        "            sentences = pickle.load(f)\n",
        "        '''\n",
        "        notParenthesesIndexes = []\n",
        "        for i, triple in enumerate(textifiedTriples):\n",
        "            if not '(' in triple:\n",
        "                notParenthesesIndexes.append(i)\n",
        "        '''\n",
        "    else:\n",
        "        if chooseTest == 'exchangedObjSubTriples':\n",
        "            results_path = exchangedObjSubTriplesResults_path\n",
        "            with open (files_path + 'textified_exchangedObjSubTriples.txt', 'rb') as f:\n",
        "                textifiedTriples = pickle.load(f)\n",
        "\n",
        "        elif chooseTest == 'randomPropertyTriples':\n",
        "            results_path = randomPropertyTriplesResults_path\n",
        "            with open (files_path + 'textified_randomPropertyTriples.txt', 'rb') as f:\n",
        "                textifiedTriples = pickle.load(f)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"chooseTest must be one of the following: 'originalTriples', 'exchangedObjSubTriples', 'randomPropertyTriples'\")\n",
        "\n",
        "\n",
        "    with open (files_path + 'triples_dev_1_split.txt', 'rb') as f:\n",
        "        triples = pickle.load(f)\n",
        "    properties_set = get_allProperties(triples)\n",
        "    triples_properties = [triple[1] for triple in triples]\n",
        "    properties_errors = {property: 0 for property in properties_set}\n",
        "\n",
        "    '''with open (files_path + 'triples_categories.txt', 'rb') as f:\n",
        "        triples_categories = pickle.load(f)'''\n",
        "\n",
        "    highestScores = []\n",
        "    # offTheShelf_highestScores = []\n",
        "\n",
        "    '''categories_set = set(triples_categories)\n",
        "    categories_errors = {category: 0 for category in categories_set}'''\n",
        "\n",
        "    classifiedResults_sum = [[0]*4 for i in range(len(thresholds))]   #[truePositives, trueNegatives, falsePositives, falseNegatives] for each threshold\n",
        "    incorrectTops_sum = 0\n",
        "    incorrectTop1_sum = 0\n",
        "    correctSentences_means = []\n",
        "    differences = []\n",
        "\n",
        "    fineTuned_errors = []\n",
        "\n",
        "    #for i in tqdm(notParenthesesIndexes):\n",
        "    for i, textifiedTriple in enumerate(tqdm(textifiedTriples)):\n",
        "    #for i in tqdm(range(textifiedTriples)):\n",
        "\n",
        "        #textifiedTriple = textifiedTriples[i]\n",
        "        correctSentences = sentences[i] if chooseTest == 'originalTriples' else None\n",
        "\n",
        "        results1 = ''\n",
        "        results2 = ''\n",
        "        if model == 'fineTuned':\n",
        "            results1_name = 'triple'+str(i)+'_results1.txt'\n",
        "            pickle_off = open(results_path + results1_name, 'rb')\n",
        "            results1 = pickle.load(pickle_off)    \n",
        "            highestScores.append(results1[0][1])\n",
        "            for j, threshold in enumerate(thresholds):\n",
        "                fineTuned_classifiedResults = classify_results(results1, threshold, correctSentences, chooseTest)\n",
        "                classifiedResults_sum[j] = [i+j for i,j in zip(classifiedResults_sum[j], fineTuned_classifiedResults)]\n",
        "\n",
        "        elif model == 'offTheShelf':\n",
        "            results2_name = 'triple'+str(i)+'_results2.txt'\n",
        "            pickle_off = open(results_path + results2_name, 'rb')\n",
        "            results2 = pickle.load(pickle_off)\n",
        "            highestScores.append(results2[0][1])\n",
        "            for j, threshold in enumerate(thresholds):\n",
        "                offTheShelf_classifiedResults = classify_results(results2, threshold, correctSentences, chooseTest)\n",
        "                classifiedResults_sum[j] = [i+j for i,j in zip(classifiedResults_sum[j], offTheShelf_classifiedResults)]\n",
        "\n",
        "\n",
        "        metrics = get_allClassificationMetrics(thresholds, classifiedResults_sum)\n",
        "\n",
        "        model_f1List = [[threshold, metrics[i][3]] for i, threshold in enumerate(thresholds)]\n",
        "        model_f1Top = sorted(model_f1List, key=lambda x: x[1], reverse=True)\n",
        "        # offTheShelf_f1List = [[threshold, offTheShelf_metrics[i][3]] for i, threshold in enumerate(thresholds)]\n",
        "        # offTheShelf_f1Top = sorted(offTheShelf_f1List, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        if chooseTest == 'originalTriples':\n",
        "            topIsCorrect = ''\n",
        "            if model == 'fineTuned':\n",
        "                topIsCorrect = top_isCorrect(results1, correctSentences)\n",
        "                correctSentencesMean = get_correctSentencesMean(results1, correctSentences, topIsCorrect)\n",
        "                if results1[0][0] not in correctSentences:\n",
        "                   incorrectTop1_sum += 1\n",
        "            elif model == 'offTheShelf':\n",
        "                topIsCorrect = top_isCorrect(results2, correctSentences)\n",
        "                correctSentencesMean = get_correctSentencesMean(results2, correctSentences, topIsCorrect)\n",
        "                if results2[0][0] not in correctSentences:\n",
        "                    incorrectTop1_sum += 1\n",
        "            \n",
        "            correctSentences_means.append(correctSentencesMean)\n",
        "\n",
        "            difference = ''\n",
        "\n",
        "            if topIsCorrect: \n",
        "                if model == 'fineTuned':\n",
        "                    if len(correctSentences) < len(results1):\n",
        "                        difference = correctSentencesMean - results1[len(correctSentences)][1]\n",
        "                    else:\n",
        "                        difference = 0.212 ##\n",
        "                elif model == 'offTheShelf':\n",
        "                    if len(correctSentences) < len(results2):\n",
        "                        difference = correctSentencesMean - results2[len(correctSentences)][1]\n",
        "                    else:\n",
        "                        difference = 0.110\n",
        "            else:\n",
        "                incorrectTops_sum += 1\n",
        "                difference = None\n",
        "                if model == 'fineTuned':\n",
        "                   fineTuned_errors.append([textifiedTriple, correctSentences, results1[:5]])\n",
        "                '''categories_errors[triples_categories[i]] += 1'''\n",
        "                properties_errors[triples_properties[i]] += 1\n",
        "\n",
        "            if model == 'fineTuned':\n",
        "                correctSentences_means.append(get_correctSentencesMean(results1, correctSentences, topIsCorrect))\n",
        "            elif model == 'offTheShelf':\n",
        "                correctSentences_means.append(get_correctSentencesMean(results2, correctSentences, topIsCorrect))\n",
        "\n",
        "            differences.append(difference)\n",
        "\n",
        "    highestScores_mean = statistics.mean(highestScores)\n",
        "\n",
        "    if chooseTest == 'originalTriples':\n",
        "        '''category_errors = [[category, categories_errors[category]] for category in categories_errors]'''\n",
        "        property_errors = [[property, properties_errors[property]] for property in properties_errors]\n",
        "\n",
        "        incorrectTops_percentage = incorrectTops_sum / n * 100\n",
        "\n",
        "        incorrectTop1_percentage = incorrectTop1_sum / n * 100\n",
        "\n",
        "        correctSentencesMeans_mean = statistics.mean(correctSentences_means)\n",
        "\n",
        "        differences_mean = statistics.mean([difference for difference in filter(None, differences)])\n",
        "\n",
        "        results = [metrics,\n",
        "                model_f1Top,\n",
        "                classifiedResults_sum,\n",
        "                incorrectTops_percentage,\n",
        "                incorrectTop1_percentage,\n",
        "                correctSentencesMeans_mean,\n",
        "                differences_mean,\n",
        "                highestScores_mean,\n",
        "                fineTuned_errors, thresholds, property_errors]\n",
        "    else:\n",
        "        results = [metrics,\n",
        "                model_f1Top,\n",
        "                classifiedResults_sum,\n",
        "                highestScores_mean,\n",
        "                thresholds]\n",
        "\n",
        "    return results\n",
        "\n",
        "def print_infoResults(results, chooseTest, model):\n",
        "    if chooseTest == 'originalTriples':\n",
        "        [metrics,\n",
        "        model_f1Top,\n",
        "        classifiedResults_sum,\n",
        "        incorrectTops_percentage,\n",
        "        incorrectTop1_percentage,\n",
        "        correctSentencesMeans_mean,\n",
        "        differences_mean,\n",
        "        highestScores_mean,\n",
        "        fineTuned_errors, thresholds, property_errors] = results\n",
        "    else:\n",
        "         [metrics,\n",
        "          model_f1Top,\n",
        "          classifiedResults_sum,\n",
        "          highestScores_mean,\n",
        "          thresholds] = results\n",
        "\n",
        "    print('CLASSIFICATION METRICS')\n",
        "    for i, threshold in enumerate(thresholds):\n",
        "        print(threshold)\n",
        "        print('\\tEvaluated model:  ')\n",
        "        print('\\t    [True Positives, False Positives]: [{:.0f}, {:.0f}]'.format(classifiedResults_sum[i][0], classifiedResults_sum[i][1]))\n",
        "        print('\\t    [False Negatives, True Negatives]: [{:.0f}, {:.0f}]'.format(classifiedResults_sum[i][3], classifiedResults_sum[i][2]))\n",
        "        print('\\t\\tPrecision: {:.3f}'.format(metrics[i][0]))\n",
        "        print('\\t\\tRecall: {:.3f}'.format(metrics[i][1]))\n",
        "        print('\\t\\taccuracy: {:.3f}'.format(metrics[i][2]))\n",
        "        print('\\t\\tf1-score: {:.3f}'.format(metrics[i][3]))\n",
        "        print()\n",
        "    print('TOP F1 THRESHOLDS')\n",
        "    print('\\tEvaluated model:')\n",
        "    print('\\t\\t', model_f1Top[:5])\n",
        "    print()\n",
        "    print('HIGHEST SCORES MEAN')\n",
        "    print('Evaluated model:  {:.5f}'.format(highestScores_mean))\n",
        "    print()\n",
        "    \n",
        "    if chooseTest == 'originalTriples':\n",
        "        print('NOT CORRECT TOPS')\n",
        "        print('Evaluated model:  {:.2f}%'.format(incorrectTops_percentage))\n",
        "        print()\n",
        "        print('NOT CORRECT TOP1')\n",
        "        print('Evaluated model:  {:.2f}%'.format(incorrectTop1_percentage))\n",
        "        print()\n",
        "        print('CORRECT SENTENCES SCORE MEANS MEAN')\n",
        "        print('Evaluated model: ', '{:.3f}'.format(correctSentencesMeans_mean))\n",
        "        print()\n",
        "        print('DIFFERENCE BETWEEN TOP SCORE MEANS AND FIRST NON CORRECT MEAN')\n",
        "        print('Evaluated model: ', '{:.3f}'.format(differences_mean))\n",
        "        print()\n",
        "        if model == 'fineTuned':\n",
        "            print('FINE-TUNED MODEL ERRORS')\n",
        "            for i, [textified_triple, correct_sentences, results] in enumerate(fineTuned_errors):\n",
        "                print(i+1, '---', textified_triple) #, '---  (category: '+category+')')\n",
        "                print('\\tCorrect Sentences')\n",
        "                for j, sentence in enumerate(correct_sentences):\n",
        "                    print('\\t\\t', j+1, sentence)\n",
        "                print('\\tTop sentences')\n",
        "                for j, result in enumerate(results):\n",
        "                    print('\\t\\t', j+1, result[0], '=>', result[1])\n",
        "                print()\n",
        "            print()\n",
        "        '''print('CATEGORY ERRORS')\n",
        "        for [category, sum] in category_errors:\n",
        "            print('\\t', category+':', sum)\n",
        "        print()'''\n",
        "        # print('PROPERTY ERRORS')\n",
        "        # for [property, sum] in property_errors:\n",
        "        #     print('\\t', property+':', sum)\n",
        "        # print()"
      ],
      "metadata": {
        "id": "5D62dsotceue"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EV-3.2 Run (INPUT NEEDED: model to evaluate)"
      ],
      "metadata": {
        "id": "dDO1kWaU5ybp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Pick one model type to evaluate\n",
        "model = 'offTheShelf'#@param ['offTheShelf', 'fineTuned']\n",
        "#'originalTriples', 'exchangedObjSubTriples', 'randomPropertyTriples'\n",
        "results = checkResults(thresholds, 401, files_path, 'originalTriples', model)\n",
        "print_infoResults(results, 'originalTriples', model)"
      ],
      "metadata": {
        "id": "hpHFpQ4acqz-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}